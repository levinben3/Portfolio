{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70c34161",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from astropy.table import Table\n",
    "from astropy.io import fits\n",
    "\n",
    "import astropy\n",
    "import scipy\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astropy import units as u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbdd4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdul = fits.open('XMM_DR14_survey.fits')\n",
    "data = hdul[1].data\n",
    "\n",
    "#Coordinates\n",
    "xmmDR14_id = data['SrcID']\n",
    "xmmDR14_detid = data['DetID']\n",
    "xmmDR14_l = data['LII']\n",
    "xmmDR14_b = data['BII']\n",
    "xmmDR14_ra = data['RA']\n",
    "xmmDR14_dec = data['Dec']\n",
    "xmmDR14_poserr = data['POSERR']\n",
    "xmmDR14_nn = data['Dist_NN']\n",
    "xmmDR14_l = np.where(xmmDR14_l < 180, xmmDR14_l, xmmDR14_l-360)\n",
    "\n",
    "#detection max likelihood\n",
    "xmmDR14_detml = data['EP_8_DET_ML']\n",
    "xmmDR14_PNdetml4 = data['PN_4_DET_ML']\n",
    "xmmDR14_PNdetml5 = data['PN_5_DET_ML']\n",
    "xmmDR14_M1detml4 = data['M1_4_DET_ML']\n",
    "xmmDR14_M1detml5 = data['M1_5_DET_ML']\n",
    "xmmDR14_M2detml4 = data['M2_4_DET_ML']\n",
    "xmmDR14_M2detml5 = data['M2_5_DET_ML']\n",
    "\n",
    "#flux\n",
    "xmmDR14_fxhalfto1 = data['EP_2_Flux']\n",
    "xmmDR14_fx1to2 = data['EP_3_Flux']\n",
    "xmmDR14_fx2to4 = data['EP_4_Flux']\n",
    "xmmDR14_fx4to12 = data['EP_5_Flux']\n",
    "xmmDR14_fxhalfto1err = data['EP_2_Flux_Err']\n",
    "xmmDR14_fx1to2err = data['EP_3_Flux_Err']\n",
    "xmmDR14_fx2to4err = data['EP_4_Flux_Err']\n",
    "xmmDR14_fx4to12err = data['EP_5_Flux_Err']\n",
    "\n",
    "#count rates\n",
    "xmmDR14_PNrthalfto1 = data['PN_2_Rate']\n",
    "xmmDR14_PNrt1to2 = data['PN_3_Rate']\n",
    "xmmDR14_PNrt2to4 = data['PN_4_Rate']\n",
    "xmmDR14_PNrt4to12 = data['PN_5_Rate']\n",
    "xmmDR14_PNrthalfto1err = data['PN_2_Rate_Err']\n",
    "xmmDR14_PNrt1to2err = data['PN_3_Rate_Err']\n",
    "xmmDR14_PNrt2to4err = data['PN_4_Rate_Err']\n",
    "xmmDR14_PNrt4to12err = data['PN_5_Rate_Err']\n",
    "xmmDR14_M1rthalfto1 = data['M1_2_Rate']\n",
    "xmmDR14_M1rt1to2 = data['M1_3_Rate']\n",
    "xmmDR14_M1rt2to4 = data['M1_4_Rate']\n",
    "xmmDR14_M1rt4to12 = data['M1_5_Rate']\n",
    "xmmDR14_M1rthalfto1err = data['M1_2_Rate_Err']\n",
    "xmmDR14_M1rt1to2err = data['M1_3_Rate_Err']\n",
    "xmmDR14_M1rt2to4err = data['M1_4_Rate_Err']\n",
    "xmmDR14_M1rt4to12err = data['M1_5_Rate_Err']\n",
    "xmmDR14_M2rthalfto1 = data['M2_2_Rate']\n",
    "xmmDR14_M2rt1to2 = data['M2_3_Rate']\n",
    "xmmDR14_M2rt2to4 = data['M2_4_Rate']\n",
    "xmmDR14_M2rt4to12 = data['M2_5_Rate']\n",
    "xmmDR14_M2rthalfto1err = data['M2_2_Rate_Err']\n",
    "xmmDR14_M2rt1to2err = data['M2_3_Rate_Err']\n",
    "xmmDR14_M2rt2to4err = data['M2_4_Rate_Err']\n",
    "xmmDR14_M2rt4to12err = data['M2_5_Rate_Err']\n",
    "\n",
    "#exposure maps\n",
    "xmmDR14_PNexphalfto1 = data['PN_2_Exp']\n",
    "xmmDR14_PNexp1to2 = data['PN_3_Exp']\n",
    "xmmDR14_PNexp2to4 = data['PN_4_Exp']\n",
    "xmmDR14_PNexp4to12 = data['PN_5_Exp']\n",
    "xmmDR14_M1exphalfto1 = data['M1_2_Exp']\n",
    "xmmDR14_M1exp1to2 = data['M1_3_Exp']\n",
    "xmmDR14_M1exp2to4 = data['M1_4_Exp']\n",
    "xmmDR14_M1exp4to12 = data['M1_5_Exp']\n",
    "xmmDR14_M2exphalfto1 = data['M2_2_Exp']\n",
    "xmmDR14_M2exp1to2 = data['M2_3_Exp']\n",
    "xmmDR14_M2exp2to4 = data['M2_4_Exp']\n",
    "xmmDR14_M2exp4to12 = data['M2_5_Exp']\n",
    "xmmDR14_EPexp4to12 = np.nansum(np.stack((xmmDR14_PNexp4to12,xmmDR14_M1exp4to12,xmmDR14_M2exp4to12)), axis=0)\n",
    "\n",
    "#flags\n",
    "xmmDR14_flag = data['Sum_Flag']\n",
    "xmmDR14_ext = data['EP_Extent']\n",
    "xmmDR14_conf = data['Confused']\n",
    "xmmDR14_PNfl = data['PN_Flag']\n",
    "xmmDR14_M1fl = data['M1_Flag']\n",
    "xmmDR14_M2fl = data['M2_Flag']\n",
    "xmmDR14_EPfl = data['EP_Flag']\n",
    "xmmDR14_PNpu = data['PN_Pileup']\n",
    "xmmDR14_M1pu = data['M1_Pileup']\n",
    "xmmDR14_M2pu = data['M2_Pileup']\n",
    "\n",
    "#supplementary data\n",
    "xmmDR14_lc = data['Tseries']\n",
    "xmmDR14_spec = data['Spectra']\n",
    "spectra_true = np.where(data['Spectra'])[0]  \n",
    "\n",
    "#multiple detections\n",
    "xmmDR14_var = data['Var_Flag']\n",
    "xmmDR14_ndet = data['N_DETECTIONS']\n",
    "xmmDR14_sra = data['SC_RA']\n",
    "xmmDR14_sdec = data['SC_Dec']\n",
    "xmmDR14_sposerr = data['SC_POSERR']\n",
    "xmmDR14_sdetml = data['SC_Det_ML']\n",
    "xmmDR14_sfxhalfto1 = data['SC_EP_2_FLUX']\n",
    "xmmDR14_sfx1to2 = data['SC_EP_3_FLUX']\n",
    "xmmDR14_sfx2to4 = data['SC_EP_4_FLUX']\n",
    "xmmDR14_sfx4to12 = data['SC_EP_5_FLUX']\n",
    "xmmDR14_sfxhalfto1err = data['SC_EP_2_FLUX_ERR']\n",
    "xmmDR14_sfx1to2err = data['SC_EP_3_FLUX_ERR']\n",
    "xmmDR14_sfx2to4err = data['SC_EP_4_FLUX_ERR']\n",
    "xmmDR14_sfx4to12err = data['SC_EP_5_FLUX_ERR']\n",
    "\n",
    "#counts\n",
    "xmmDR14_PNctshalfto1 = xmmDR14_PNrthalfto1*xmmDR14_PNexphalfto1\n",
    "xmmDR14_PNcts1to2 = xmmDR14_PNrt1to2*xmmDR14_PNexp1to2\n",
    "xmmDR14_PNcts2to4 = xmmDR14_PNrt2to4*xmmDR14_PNexp2to4\n",
    "xmmDR14_PNcts4to12 = xmmDR14_PNrt4to12*xmmDR14_PNexp4to12\n",
    "xmmDR14_M1ctshalfto1 = xmmDR14_M1rthalfto1*xmmDR14_M1exphalfto1\n",
    "xmmDR14_M1cts1to2 = xmmDR14_M1rt1to2*xmmDR14_M1exp1to2\n",
    "xmmDR14_M1cts2to4 = xmmDR14_M1rt2to4*xmmDR14_M1exp2to4\n",
    "xmmDR14_M1cts4to12 = xmmDR14_M1rt4to12*xmmDR14_M1exp4to12\n",
    "xmmDR14_M2ctshalfto1 = xmmDR14_M2rthalfto1*xmmDR14_M2exphalfto1\n",
    "xmmDR14_M2cts1to2 = xmmDR14_M2rt1to2*xmmDR14_M2exp1to2\n",
    "xmmDR14_M2cts2to4 = xmmDR14_M2rt2to4*xmmDR14_M2exp2to4\n",
    "xmmDR14_M2cts4to12 = xmmDR14_M2rt4to12*xmmDR14_M2exp4to12\n",
    "xmmDR14_EPcts1to2 = np.nansum(np.stack((xmmDR14_PNcts4to12,xmmDR14_M1cts4to12,xmmDR14_M2cts4to12)), axis=0)\n",
    "xmmDR14_EPcts2to4 = np.nansum(np.stack((xmmDR14_PNcts2to4,xmmDR14_M1cts2to4,xmmDR14_M2cts2to4)), axis=0)\n",
    "xmmDR14_EPcts4to12 = np.nansum(np.stack((xmmDR14_PNcts1to2,xmmDR14_M1cts1to2,xmmDR14_M2cts1to2)), axis=0)\n",
    "\n",
    "#chi squared\n",
    "xmmDR14_PNchi2prob = data['PN_Chi2prob']\n",
    "xmmDR14_M1chi2prob = data['M1_Chi2prob']\n",
    "xmmDR14_M2chi2prob = data['M2_Chi2prob']\n",
    "\n",
    "#8 cts\n",
    "xmmDR14_8cts = data['EP_8_Cts']\n",
    "xmmDR14_PN8cts = data['PN_8_Cts']\n",
    "xmmDR14_M18cts = data['M1_8_Cts']\n",
    "xmmDR14_M28cts = data['M2_8_Cts']\n",
    "hdul.close()\n",
    "\n",
    "xmmDR14_EPhr4 = data['EP_HR4']\n",
    "xmmDR14_EPhr3 = data['EP_HR3']\n",
    "xmmDR14_EPhr2 = data['EP_HR2']\n",
    "xmmDR14_PNhr4 = data['PN_HR4']\n",
    "xmmDR14_M1hr4 = data['M1_HR4']\n",
    "xmmDR14_M2hr4 = data['M2_HR4']\n",
    "\n",
    "xmmDR14_PNFvar = data['PN_Fvar']\n",
    "xmmDR14_M1Fvar = data['M1_Fvar']\n",
    "xmmDR14_M2Fvar = data['M2_Fvar']\n",
    "xmmDR14_Varflag = data['Var_Flag']\n",
    "xmmDR14_SC_Varflag = data['SC_Var_Flag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d2a831",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add up EPIC PN/MOS1/MOS2 count rates\n",
    "xmmDR14_EPrt2to4 = np.nansum(np.stack((xmmDR14_PNrt2to4,xmmDR14_M1rt2to4,xmmDR14_M2rt2to4)), axis=0)\n",
    "xmmDR14_EPrt4to12 = np.nansum(np.stack((xmmDR14_PNrt4to12,xmmDR14_M1rt4to12,xmmDR14_M2rt4to12)), axis=0)\n",
    "xmmDR14_EPrt2to12 = np.nansum(np.stack((xmmDR14_EPrt2to4,xmmDR14_EPrt4to12)), axis=0)\n",
    "\n",
    "#add errors in quadrature\n",
    "xmmDR14_EPrt2to4_err = np.sqrt(np.nansum(np.stack((xmmDR14_PNrt2to4err**2,xmmDR14_M1rt2to4err**2,xmmDR14_M2rt2to4err**2)), axis=0))\n",
    "xmmDR14_EPrt4to12_err = np.sqrt(np.nansum(np.stack((xmmDR14_PNrt4to12err**2,xmmDR14_M1rt4to12err**2,xmmDR14_M2rt4to12err**2)), axis=0))\n",
    "xmmDR14_EPcts2to4_err = np.sqrt(np.nansum(np.stack(((xmmDR14_PNrt2to4err*xmmDR14_PNexp2to4)**2,(xmmDR14_M1rt2to4err*xmmDR14_M1exp2to4)**2,(xmmDR14_M2rt2to4err*xmmDR14_M2exp2to4)**2)), axis=0))\n",
    "xmmDR14_EPcts4to12_err = np.sqrt(np.nansum(np.stack(((xmmDR14_PNrt4to12err*xmmDR14_PNexp4to12)**2,(xmmDR14_M1rt4to12err*xmmDR14_M1exp4to12)**2,(xmmDR14_M2rt4to12err*xmmDR14_M2exp4to12)**2)), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bbd226",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sgr A*\n",
    "sgra_ra, sgra_dec = 266.41683708333335, -29.007810555555555\n",
    "sgrrad = np.sqrt((sgra_ra-xmmDR14_ra)**2 + (sgra_dec-xmmDR14_dec)**2) #approximate distance to Sgr A*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73757fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate APPROXIMATE distance (accurate for small distances)\n",
    "def dist(ra1, dec1, ra2, dec2):\n",
    "    return(np.sqrt((ra2-ra1)**2 + (dec2-dec1)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0df544",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stray light sources\n",
    "#GX 3+1\n",
    "gxra, gxdec = 266.983330, -26.563610\n",
    "# second stray light source: 354.28, -0.17\n",
    "sl2ra, sl2dec = 262.9945537, -33.8641678\n",
    "sl3ra, sl3dec = 270.25, -25.05 #estimated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be599059",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hr(h, s):\n",
    "    return((h-s)/(h+s))\n",
    "\n",
    "#hardness ratio: (4to12 - 2to4)/(4to12 + 2to4) [similar to HR2, except 4to12 vs 4to8]\n",
    "PNhr2 = hr(xmmDR14_PNrt4to12, xmmDR14_PNrt2to4)\n",
    "M1hr2 = hr(xmmDR14_M1rt4to12, xmmDR14_M1rt2to4)\n",
    "M2hr2 = hr(xmmDR14_M2rt4to12, xmmDR14_M2rt2to4)\n",
    "#combined EPIC HR\n",
    "EPhr2_avg = np.nanmean(np.vstack((PNhr2, M1hr2, M2hr2)), axis=0)\n",
    "EPhr2 = hr(np.nansum(np.stack((xmmDR14_PNrt4to12,xmmDR14_M1rt4to12,xmmDR14_M2rt4to12)), axis=0), \n",
    "          np.nansum(np.stack((xmmDR14_PNrt2to4,xmmDR14_M1rt2to4,xmmDR14_M2rt2to4)), axis=0))\n",
    "#EPhr2 = hr(xmmDR14_PNrt4to12+xmmDR14_M1rt4to12+xmmDR14_M2rt4to12, xmmDR14_PNrt2to4+xmmDR14_M1rt2to4+xmmDR14_M2rt2to4)\n",
    "\n",
    "#hardness ratio: (2to12 - 0.5to2)/(2to12 + 0.5to2) \n",
    "PNhr1 = hr(xmmDR14_PNrt4to12+xmmDR14_PNrt2to4,xmmDR14_PNrthalfto1+xmmDR14_PNrt1to2)\n",
    "M1hr1 = hr(xmmDR14_M1rt4to12+xmmDR14_M1rt2to4,xmmDR14_M1rthalfto1+xmmDR14_M1rt1to2)\n",
    "M2hr1 = hr(xmmDR14_M2rt4to12+xmmDR14_M2rt2to4,xmmDR14_M2rthalfto1+xmmDR14_M2rt1to2)\n",
    "#combined EPIC HR\n",
    "EPhr1 = hr(np.nansum(np.stack((xmmDR14_PNrt4to12,xmmDR14_M1rt4to12,xmmDR14_M2rt4to12,xmmDR14_PNrt2to4,xmmDR14_M1rt2to4,xmmDR14_M2rt2to4)), axis=0), \n",
    "          np.nansum(np.stack((xmmDR14_PNrthalfto1+xmmDR14_PNrt1to2+xmmDR14_M1rthalfto1+xmmDR14_M1rt1to2+xmmDR14_M2rthalfto1+xmmDR14_M2rt1to2)), axis=0))\n",
    "#hr(xmmDR14_PNrt4to12+xmmDR14_PNrt2to4+xmmDR14_M1rt4to12+xmmDR14_M1rt2to4+xmmDR14_M2rt4to12+xmmDR14_M2rt2to4,xmmDR14_PNrthalfto1+xmmDR14_PNrt1to2+xmmDR14_M1rthalfto1+xmmDR14_M1rt1to2+xmmDR14_M2rthalfto1+xmmDR14_M2rt1to2)\n",
    "\n",
    "#Muno: H is 2.0-3.3keV, L is 0.5-2.0keV; foreground is -1 < hr0 < -0.175.  Here: H is 2-4 keV\n",
    "PNhr0 = hr(xmmDR14_PNrt2to4,xmmDR14_PNrthalfto1+xmmDR14_PNrt1to2)\n",
    "M1hr0 = hr(xmmDR14_M1rt2to4,xmmDR14_M1rthalfto1+xmmDR14_M1rt1to2)\n",
    "M2hr0 = hr(xmmDR14_M2rt2to4,xmmDR14_M2rthalfto1+xmmDR14_M2rt1to2)\n",
    "#combined EPIC HR\n",
    "EPhr0 = hr(np.nansum(np.stack((xmmDR14_PNrt2to4,xmmDR14_M1rt2to4,xmmDR14_M2rt2to4)), axis=0), \n",
    "          np.nansum(np.stack((xmmDR14_PNrthalfto1+xmmDR14_PNrt1to2+xmmDR14_M1rthalfto1+xmmDR14_M1rt1to2+xmmDR14_M2rthalfto1+xmmDR14_M2rt1to2)), axis=0))\n",
    "#hr(xmmDR14_PNrt2to4+xmmDR14_M1rt2to4+xmmDR14_M2rt2to4,xmmDR14_PNrthalfto1+xmmDR14_PNrt1to2+xmmDR14_M1rthalfto1+xmmDR14_M1rt1to2+xmmDR14_M2rthalfto1+xmmDR14_M2rt1to2)\n",
    "\n",
    "def hr_err(h,s, dh, ds):\n",
    "    hr_0 = hr(h, s)\n",
    "    hr_h = hr(h+dh, s)\n",
    "    hr_s = hr(h, s+ds)\n",
    "    return(np.sqrt((hr_s-hr_0)**2 + (hr_h-hr_0)**2))\n",
    "\n",
    "#use counts to define uncertainty\n",
    "#combine EPIC counts\n",
    "Hstack = np.nansum(np.stack((xmmDR14_PNcts4to12,xmmDR14_M1cts4to12,xmmDR14_M2cts4to12)), axis=0)\n",
    "Sstack = np.nansum(np.stack((xmmDR14_PNcts2to4,xmmDR14_M1cts2to4,xmmDR14_M2cts2to4)), axis=0)\n",
    "dHstack = xmmDR14_EPcts4to12_err\n",
    "dSstack = xmmDR14_EPcts2to4_err\n",
    "hr2err = hr_err(Hstack, Sstack, dHstack, dSstack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9defb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initial filter for flagged/extended sources\n",
    "xmmDR14_flfilt = np.where((xmmDR14_flag == 0) & (xmmDR14_ext == 0) & (xmmDR14_conf == False))[0]\n",
    "\n",
    "#calculate nearest neighbor\n",
    "nndist_all = np.zeros(np.size(xmmDR14_ra)) #distance to nearest neighbor in arcsec\n",
    "for i in range(np.size(xmmDR14_ra)):\n",
    "    idist = dist(xmmDR14_ra, xmmDR14_dec, xmmDR14_ra[i], xmmDR14_dec[i])\n",
    "    nndist_all[i] = 3600*idist[np.argsort(idist)[1]]\n",
    "    \n",
    "#filter out duplicates:\n",
    "\n",
    "nndist_min = 6    #min distance [arcsec] for two sources to be considered distinct\n",
    "\n",
    "duplist = np.zeros(np.size(xmmDR14_ndet))  #check for duplicates & find longest exposure (discard others)\n",
    "for i in range(np.size(xmmDR14_flfilt)):\n",
    "    if (xmmDR14_ndet[xmmDR14_flfilt[i]] > 1) and (duplist[xmmDR14_flfilt[i]] == 0):\n",
    "        ilist = np.where(xmmDR14_id == xmmDR14_id[xmmDR14_flfilt[i]])[0]  #list of all entries with same src id\n",
    "        jlist = np.where((xmmDR14_flag[ilist] == 0) & (xmmDR14_ext[ilist] == 0) & (xmmDR14_conf[ilist] == False))[0]\n",
    "        ibest = np.argmax(xmmDR14_EPexp4to12[ilist[jlist]])  #select entry w/ highest exposure map\n",
    "        #print(ibest)\n",
    "        duplist[ilist] = 1\n",
    "        duplist[ilist[jlist[ibest]]] = 10\n",
    "        \n",
    "#preliminary filter        \n",
    "glist_prelim = np.where((xmmDR14_flag == 0) & (xmmDR14_ext == 0) & (xmmDR14_conf == False) & \n",
    "                         ((xmmDR14_ndet == 1) | (duplist == 10)))[0]\n",
    "    \n",
    "#re-calculate NN for \"filtered\" sources\n",
    "nndist_filt = np.zeros(np.size(glist_prelim)) #distance to nearest neighbor in arcsec\n",
    "for i in range(np.size(glist_prelim)):\n",
    "    idist = dist(xmmDR14_ra[glist_prelim], xmmDR14_dec[glist_prelim], xmmDR14_ra[glist_prelim][i], xmmDR14_dec[glist_prelim][i])\n",
    "    nndist_filt[i] = 3600*idist[np.argsort(idist)[1]]\n",
    "\n",
    "duplist2 = np.zeros(np.size(glist_prelim))\n",
    "for i in range(np.size(glist_prelim)):\n",
    "    if nndist_filt[i] < nndist_min:\n",
    "        ilist2 = np.where(3600*dist(xmmDR14_ra[glist_prelim], xmmDR14_dec[glist_prelim], xmmDR14_ra[glist_prelim][i], xmmDR14_dec[glist_prelim][i]) < nndist_min)[0]  #list of filtered entries with distance < min, including self\n",
    "        ibest2 = np.argmax(xmmDR14_EPexp4to12[glist_prelim][ilist2])  #select entry w/ highest exposure map\n",
    "        #print(ibest)\n",
    "        duplist2[ilist2] = 1\n",
    "        duplist2[ilist2[ibest2]] = 10\n",
    "    \n",
    "#final \"clean\" source list\n",
    "x = glist_prelim[np.where((nndist_filt > 6) | (duplist2 == 10))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f022e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = x[np.in1d(x, spectra_true)]\n",
    "fx2to4 = xmmDR14_fx2to4[z]\n",
    "fx4to12 = xmmDR14_fx4to12[z]\n",
    "\n",
    "mask = (fx2to4 != 0) | (fx4to12 != 0)\n",
    "print(len(mask))\n",
    "y = z[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f080fe98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.io import fits\n",
    "from astropy.table import Table\n",
    "len(y)\n",
    "hdul1 = fits.open('XMM_DR14_survey.fits')\n",
    "tbl = Table(hdul1[1].data)          # now you have an Astropy Table\n",
    "hdr = hdul1[1].header               # save the header if you like\n",
    "hdul1.close()\n",
    "filtered = tbl[y]\n",
    "filtered.write('filtered_XMM_DR14.fits',\n",
    "               format='fits',\n",
    "               overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09aa5241",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdul2 = fits.open('filtered_XMM_DR14.fits')\n",
    "data2 = hdul2[1].data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35974993",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PNdetml4 = xmmDR14_PNdetml4[y]\n",
    "PNdetml5 = xmmDR14_PNdetml5[y]\n",
    "M1detml4 = xmmDR14_M1detml4[y]\n",
    "M1detml5 = xmmDR14_M1detml5[y]\n",
    "M2detml4 = xmmDR14_M2detml4[y]\n",
    "M2detml5 = xmmDR14_M2detml5[y]\n",
    "PNchi2prob = xmmDR14_PNchi2prob[y]\n",
    "M1chi2prob = xmmDR14_M1chi2prob[y]\n",
    "M2chi2prob = xmmDR14_M2chi2prob[y]\n",
    "xmm_8cts = xmmDR14_8cts[y]\n",
    "PN8cts = xmmDR14_PN8cts[y]\n",
    "M18cts = xmmDR14_M18cts[y]\n",
    "M28cts = xmmDR14_M28cts[y]\n",
    "\n",
    "fxhalfto1 = xmmDR14_fxhalfto1[y]\n",
    "fx1to2 = xmmDR14_fx1to2[y]\n",
    "fx2to4 = xmmDR14_fx2to4[y]\n",
    "fx4to12 = xmmDR14_fx4to12[y]\n",
    "SrcID_filt=xmmDR14_id[y]\n",
    "DetID_filt=xmmDR14_detid[y]\n",
    "xmmDR14_obsid=data['OBS_ID']\n",
    "ObsID_filt=xmmDR14_obsid[y]\n",
    "l_filt = xmmDR14_l[y]\n",
    "b_filt = xmmDR14_b[y]\n",
    "sra = xmmDR14_sra[y]\n",
    "sdec = xmmDR14_sdec[y]\n",
    "sposerr = xmmDR14_sposerr[y]\n",
    "\n",
    "fx2to12 = []\n",
    "for i,data in enumerate(y):\n",
    "    fx2to12.append(fx2to4[i] + fx4to12[i])\n",
    "    if fx2to12[i] == 0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b56b8630",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load all points with coordinates\n",
    "import numpy as np\n",
    "source_fits  = 'filtered_XMM_DR14.fits'   # catalog with GLON, GLAT columns\n",
    "src    = Table.read(source_fits)\n",
    "# Option A: raw floats → N×2 array\n",
    "points = [(float(l), float(b)) for l, b in zip(src['LII'], src['BII'])]\n",
    "len(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275e648f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill in gaps in nH file\n",
    "from astropy.io import fits\n",
    "import numpy as np\n",
    "from scipy.interpolate import griddata\n",
    "\n",
    "# 1) Load your map\n",
    "hdu = fits.open('nH_analysis/column_density_inner20deg.fits')\n",
    "data = hdu[0].data\n",
    "hdr  = hdu[0].header\n",
    "\n",
    "# 2) Build a mask of the NaNs\n",
    "y, x = np.indices(data.shape)\n",
    "mask = np.isnan(data)\n",
    "\n",
    "# 3) Interpolate the NaNs from their valid neighbors\n",
    "#    Here we use nearest‐neighbor to avoid overshoot; \n",
    "#    you can choose 'linear' or 'cubic' if you like.\n",
    "filled_vals = griddata(\n",
    "    (x[~mask], y[~mask]), \n",
    "    data[~mask], \n",
    "    (x[mask], y[mask]),\n",
    "    method='nearest'\n",
    ")\n",
    "data[mask] = filled_vals\n",
    "\n",
    "# 4) Write out a new FITS with no NaNs\n",
    "hdu[0].data = data\n",
    "hdu.writeto('nh_map_filled.fits', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f8624a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#absorption contour grouping\n",
    "import numpy as np\n",
    "from astropy.io import fits\n",
    "from matplotlib.path import Path\n",
    "import os\n",
    "\n",
    "def load_contours(fits_path):\n",
    "    with fits.open(fits_path) as hdul:\n",
    "        region_hdu = hdul['REGION']\n",
    "        contour_data = region_hdu.data\n",
    "        \n",
    "        contours = []\n",
    "        for i, row in enumerate(contour_data):\n",
    "            xs = row['X']\n",
    "            ys = row['Y']\n",
    "            \n",
    "            # Apply WCS transformation - USE YOUR ACTUAL CRPIX VALUES\n",
    "            l_coords = -0.0032 * (np.array(xs) - 3164.0)  # Adjust CRPIX1\n",
    "            b_coords = 0.0032 * (np.array(ys) - 319.0)    # Adjust CRPIX2\n",
    "            \n",
    "            vertices = np.column_stack((l_coords, b_coords))\n",
    "            if len(vertices) > 0 and not np.array_equal(vertices[0], vertices[-1]):\n",
    "                vertices = np.vstack([vertices, vertices[0]])\n",
    "            \n",
    "            contours.append({\n",
    "                'index': i,\n",
    "                'level': row['CONTOUR_LEVEL'],\n",
    "                'vertices': vertices,\n",
    "                'vertex_count': len(vertices),\n",
    "                'sources': []\n",
    "            })\n",
    "    \n",
    "    return contours\n",
    "\n",
    "def normalize_coordinates(sources):\n",
    "    normalized_sources = []\n",
    "    for source_l, source_b in sources:\n",
    "        if source_l > 180:\n",
    "            norm_l = source_l - 360\n",
    "        else:\n",
    "            norm_l = source_l\n",
    "        normalized_sources.append((norm_l, source_b))\n",
    "    return normalized_sources\n",
    "\n",
    "\n",
    "def assign_sources_to_contours(sources, contours):\n",
    "    normalized_sources = normalize_coordinates(sources)\n",
    "    unassigned = []\n",
    "    \n",
    "    # CORRECTED SORTING: Highest level first, then smallest contours first\n",
    "    sorted_contours = sorted(contours, key=lambda c: (-c['level'], c['vertex_count']))\n",
    "    assigned = [False] * len(sources)\n",
    "    \n",
    "    for contour in sorted_contours:\n",
    "        path = Path(contour['vertices'])\n",
    "        \n",
    "        for source_idx, (source_l, source_b) in enumerate(sources):\n",
    "            if assigned[source_idx]:\n",
    "                continue\n",
    "                \n",
    "            norm_l, norm_b = normalized_sources[source_idx]\n",
    "            if path.contains_point((norm_l, norm_b)):\n",
    "                contour['sources'].append(source_idx)\n",
    "                assigned[source_idx] = True\n",
    "    \n",
    "    unassigned = [i for i, status in enumerate(assigned) if not status]\n",
    "    return unassigned\n",
    "\n",
    "def create_ds9_region_files(contours, sources, unassigned, output_dir=\"ds9_contour_regions\"):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    colors = ['red', 'green', 'blue', 'cyan', 'magenta', 'yellow', 'orange', \n",
    "              'pink', 'lime', 'purple', 'brown', 'gray', 'olive', 'navy', 'teal']\n",
    "    \n",
    "    region_files = []\n",
    "    \n",
    "    for i, contour in enumerate(contours):\n",
    "        if not contour['sources']:\n",
    "            continue\n",
    "            \n",
    "        color = colors[i % len(colors)]\n",
    "        level_str = f\"{contour['level']:.2e}\".replace('.', 'p').replace('+', '')\n",
    "        filename = os.path.join(output_dir, f\"contour_{contour['index']}_level_{level_str}.reg\")\n",
    "        \n",
    "        with open(filename, 'w') as f:\n",
    "            f.write(\"# Region file format: DS9 version 4.1\\n\")\n",
    "            f.write(f\"global color={color} dashlist=8 3 width=1 font=\\\"helvetica 10 normal roman\\\" \")\n",
    "            f.write(\"select=1 highlite=1 dash=0 fixed=0 edit=1 move=1 delete=1 include=1 source=1\\n\")\n",
    "            f.write(\"galactic\\n\")\n",
    "            \n",
    "            for source_idx in contour['sources']:\n",
    "                l, b = sources[source_idx]\n",
    "                f.write(f'circle({l:.6f},{b:.6f},10\") # text={{S{source_idx}}}\\n')\n",
    "        \n",
    "        region_files.append(filename)\n",
    "        print(f\"Created {filename} with {len(contour['sources'])} sources\")\n",
    "    \n",
    "    if unassigned:\n",
    "        filename = os.path.join(output_dir, \"unassigned_sources.reg\")\n",
    "        with open(filename, 'w') as f:\n",
    "            f.write(\"# Region file format: DS9 version 4.1\\n\")\n",
    "            f.write(\"global color=white dashlist=8 3 width=2 font=\\\"helvetica 10 normal roman\\\" \")\n",
    "            f.write(\"select=1 highlite=1 dash=0 fixed=0 edit=1 move=1 delete=1 include=1 source=1\\n\")\n",
    "            f.write(\"galactic\\n\")\n",
    "            \n",
    "            for source_idx in unassigned:\n",
    "                l, b = sources[source_idx]\n",
    "                f.write(f'circle({l:.6f},{b:.6f},10\") # text={{U{source_idx}}}\\n')\n",
    "        \n",
    "        region_files.append(filename)\n",
    "        print(f\"Created {filename} with {len(unassigned)} unassigned sources\")\n",
    "    \n",
    "    return region_files\n",
    "\n",
    "\n",
    "def add_contour_index_to_table(input_fits, output_fits, contour_assignments):\n",
    "    \"\"\"Add contour index column to FITS table and save as a new file.\"\"\"\n",
    "    with fits.open(input_fits) as hdul:\n",
    "        table = hdul[1].data\n",
    "        columns = hdul[1].columns\n",
    "\n",
    "        # Create array for contour indices (default -1 for unassigned)\n",
    "        contour_indices = np.full(len(table), -1, dtype=int)\n",
    "        for contour in contour_assignments:\n",
    "            for source_idx in contour['sources']:\n",
    "                contour_indices[source_idx] = contour['index']\n",
    "\n",
    "        # Add new column to table\n",
    "        new_col = fits.Column(name='contour_index', format='J', array=contour_indices)\n",
    "        new_hdu = fits.BinTableHDU.from_columns(columns + new_col, header=hdul[1].header)\n",
    "\n",
    "        # Write to new FITS file\n",
    "        hdul_new = fits.HDUList([hdul[0], new_hdu])\n",
    "        hdul_new.writeto(output_fits, overwrite=True)\n",
    "        print(f\"Saved new FITS table with 'contour_index' column to {output_fits}\") \n",
    "\n",
    "def check_coordinate_consistency(sources, contours):\n",
    "    # Extract source longitude ranges\n",
    "    source_lons = [coord[0] for coord in sources]\n",
    "    source_min_lon = min(source_lons)\n",
    "    source_max_lon = max(source_lons)\n",
    "    \n",
    "    # Extract contour longitude ranges\n",
    "    contour_lons = []\n",
    "    for contour in contours:\n",
    "        lons = contour['vertices'][:,0]\n",
    "        contour_lons.extend(lons)\n",
    "    contour_min_lon = min(contour_lons)\n",
    "    contour_max_lon = max(contour_lons)\n",
    "    \n",
    "    print(f\"Source longitude range: {source_min_lon:.2f} to {source_max_lon:.2f}\")\n",
    "    print(f\"Contour longitude range: {contour_min_lon:.2f} to {contour_max_lon:.2f}\")\n",
    "    \n",
    "    # Check if ranges overlap\n",
    "    overlap = (source_min_lon <= contour_max_lon) and (source_max_lon >= contour_min_lon)\n",
    "    if overlap:\n",
    "        print(\"Coordinate systems appear consistent.\")\n",
    "    else:\n",
    "        print(\"WARNING: Coordinate systems are inconsistent!\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Configuration - REPLACE WITH YOUR DATA\n",
    "    contour_file = \"nH_analysis/test_contour_inner20deg_convolved.fits\"\n",
    "    fits_table = \"filtered_XMM_DR14.fits\"  # Path to your source table\n",
    "    sources = points\n",
    "    output_dir = \"ds9_contour_regions_test\"\n",
    "    \n",
    "    # Step 1: Load contours\n",
    "    contours = load_contours(contour_file)\n",
    "    print(f\"Loaded {len(contours)} contours\")\n",
    "    check_coordinate_consistency(sources, contours)\n",
    "\n",
    "    # Step 2: Assign sources to contours\n",
    "    unassigned = assign_sources_to_contours(sources, contours)\n",
    "    assigned_count = sum(len(c['sources']) for c in contours)\n",
    "    print(f\"Assigned {assigned_count} sources to contours\")\n",
    "    print(f\"Unassigned sources: {len(unassigned)}\")\n",
    "    \n",
    "    # Step 3: Create DS9 region files\n",
    "    region_files = create_ds9_region_files(contours, sources, unassigned, output_dir)\n",
    "    print(f\"\\nCreated {len(region_files)} region files in '{output_dir}'\")\n",
    "    \n",
    "    # Step 4: Add contour index to FITS table\n",
    "    add_contour_index_to_table(fits_table, \"filtered_XMM_DR14_with_contour_test.fits\", contours)\n",
    "    print(\"Contour indices added to FITS table\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b31e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extinction contour grouping\n",
    "import numpy as np\n",
    "from astropy.io import fits\n",
    "from matplotlib.path import Path\n",
    "import os\n",
    "\n",
    "def load_contours(fits_path):\n",
    "    with fits.open(fits_path) as hdul:\n",
    "        contour_ext = next((hdu for hdu in hdul if hdu.name == 'REGION'), hdul[1])\n",
    "        contour_data = contour_ext.data\n",
    "        header = contour_ext.header\n",
    "        \n",
    "        # Extract WCS parameters\n",
    "        crpix1 = header.get('TCRPX11', 1020.50)\n",
    "        crpix2 = header.get('TCRPX12', 102.50)\n",
    "        cdelt1 = header.get('TCDLT11', -0.010)\n",
    "        cdelt2 = header.get('TCDLT12', 0.010)\n",
    "        \n",
    "        contours = []\n",
    "        for i, row in enumerate(contour_data):\n",
    "            xs = row['X']\n",
    "            ys = row['Y']\n",
    "            \n",
    "            # Apply WCS transformation\n",
    "            lons = cdelt1 * (np.array(xs) - crpix1)\n",
    "            lats = cdelt2 * (np.array(ys) - crpix2)\n",
    "            \n",
    "            # Normalize longitude\n",
    "            lons = (lons + 180) % 360 - 180\n",
    "            \n",
    "            vertices = np.column_stack((lons, lats))\n",
    "            contours.append({\n",
    "                'index': i,\n",
    "                'level': row['CONTOUR_LEVEL'],\n",
    "                'vertices': vertices,\n",
    "                'sources': []\n",
    "            })\n",
    "    return contours\n",
    "\n",
    "def normalize_sources(sources):\n",
    "    return [((l + 180) % 360 - 180, b) for l, b in sources]\n",
    "\n",
    "def assign_sources_to_contours(sources, contours):\n",
    "    assigned = [False] * len(sources)\n",
    "    sorted_contours = sorted(contours, key=lambda c: (-c['level'], len(c['vertices'])))\n",
    "    \n",
    "    for contour in sorted_contours:\n",
    "        path = Path(contour['vertices'])\n",
    "        for src_idx, (l, b) in enumerate(sources):\n",
    "            if not assigned[src_idx] and path.contains_point((l, b)):\n",
    "                contour['sources'].append(src_idx)\n",
    "                assigned[src_idx] = True\n",
    "    return [i for i, status in enumerate(assigned) if not status]\n",
    "\n",
    "def create_ds9_region_files(contours, sources, unassigned, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    colors = ['red', 'green', 'blue', 'cyan', 'magenta', 'yellow', 'orange', \n",
    "              'pink', 'lime', 'purple', 'brown', 'gray', 'olive', 'navy', 'teal']\n",
    "    \n",
    "    region_files = []\n",
    "    \n",
    "    for i, contour in enumerate(contours):\n",
    "        if not contour['sources']:\n",
    "            continue\n",
    "            \n",
    "        color = colors[i % len(colors)]\n",
    "        level_str = f\"{contour['level']:.2e}\".replace('.', 'p').replace('+', '')\n",
    "        filename = os.path.join(output_dir, f\"contour_{contour['index']}_level_{level_str}.reg\")\n",
    "        \n",
    "        with open(filename, 'w') as f:\n",
    "            f.write(\"# Region file format: DS9 version 4.1\\n\")\n",
    "            f.write(f\"global color={color} dashlist=8 3 width=1 font=\\\"helvetica 10 normal roman\\\" \")\n",
    "            f.write(\"select=1 highlite=1 dash=0 fixed=0 edit=1 move=1 delete=1 include=1 source=1\\n\")\n",
    "            f.write(\"galactic\\n\")\n",
    "            \n",
    "            for source_idx in contour['sources']:\n",
    "                l, b = sources[source_idx]\n",
    "                f.write(f'circle({l:.6f},{b:.6f},10\") # text={{S{source_idx}}}\\n')\n",
    "        \n",
    "        region_files.append(filename)\n",
    "        print(f\"Created {filename} with {len(contour['sources'])} sources\")\n",
    "    \n",
    "    if unassigned:\n",
    "        filename = os.path.join(output_dir, \"unassigned_sources.reg\")\n",
    "        with open(filename, 'w') as f:\n",
    "            f.write(\"# Region file format: DS9 version 4.1\\n\")\n",
    "            f.write(\"global color=white dashlist=8 3 width=2 font=\\\"helvetica 10 normal roman\\\" \")\n",
    "            f.write(\"select=1 highlite=1 dash=0 fixed=0 edit=1 move=1 delete=1 include=1 source=1\\n\")\n",
    "            f.write(\"galactic\\n\")\n",
    "            \n",
    "            for source_idx in unassigned:\n",
    "                l, b = sources[source_idx]\n",
    "                f.write(f'circle({l:.6f},{b:.6f},10\") # text={{U{source_idx}}}\\n')\n",
    "        \n",
    "        region_files.append(filename)\n",
    "        print(f\"Created {filename} with {len(unassigned)} unassigned sources\")\n",
    "    \n",
    "    return region_files\n",
    "\n",
    "def add_contour_index_to_table(input_fits, output_fits, contour_assignments):\n",
    "    \"\"\"Add contour index column to FITS table and save as a new file.\"\"\"\n",
    "    with fits.open(input_fits) as hdul:\n",
    "        table = hdul[1].data\n",
    "        columns = hdul[1].columns\n",
    "\n",
    "        # Create array for contour indices (default -1 for unassigned)\n",
    "        contour_indices = np.full(len(table), -1, dtype=int)\n",
    "        for contour in contour_assignments:\n",
    "            for source_idx in contour['sources']:\n",
    "                contour_indices[source_idx] = contour['index']\n",
    "\n",
    "        # Add new column to table\n",
    "        new_col = fits.Column(name='contour_index', format='J', array=contour_indices)\n",
    "        new_hdu = fits.BinTableHDU.from_columns(columns + new_col, header=hdul[1].header)\n",
    "\n",
    "        # Write to new FITS file\n",
    "        hdul_new = fits.HDUList([hdul[0], new_hdu])\n",
    "        hdul_new.writeto(output_fits, overwrite=True)\n",
    "        print(f\"Saved new FITS table with 'contour_index' column to {output_fits}\") \n",
    "        \n",
    "def main():\n",
    "    # Configuration\n",
    "    contour_file = \"extinction_analysis/VVVextmap_mef_contour_perfect.fits\"\n",
    "    source_file = \"filtered_XMM_DR14.fits\"  # Path to source catalog\n",
    "    sources = points  # Your source list (if loaded separately)\n",
    "    output_dir = \"ds9_contour_regions_extinction_perfect\"\n",
    "    output_fits = \"extinction_analysis/filtered_XMM_DR14_with_contour.fits\"\n",
    "    \n",
    "    # Step 1: Load contours\n",
    "    contours = load_contours(contour_file)\n",
    "    print(f\"Loaded {len(contours)} contours\")\n",
    "    \n",
    "    # Step 2: Normalize sources\n",
    "    sources_normalized = normalize_sources(sources)\n",
    "    \n",
    "    # Step 3: Assign sources\n",
    "    unassigned = assign_sources_to_contours(sources_normalized, contours)\n",
    "    assigned_count = sum(len(c['sources']) for c in contours)\n",
    "    print(f\"Assigned {assigned_count} sources to contours\")\n",
    "    print(f\"Unassigned sources: {len(unassigned)}\")\n",
    "    \n",
    "    # Step 4: Create region files with colors\n",
    "    region_files = create_ds9_region_files(contours, sources_normalized, unassigned, output_dir)\n",
    "    print(f\"\\nCreated {len(region_files)} region files in '{output_dir}'\")\n",
    "    # Step 5: Update source catalog\n",
    "    add_contour_index_to_table(source_file, output_fits, contours)\n",
    "    \n",
    "    # Step 6: Galactic Center verification\n",
    "    #gc_contour = None\n",
    "   # for contour in contours:\n",
    "    #    if any(0.0 == sources_normalized[src_idx][0] and 0.0 == sources_normalized[src_idx][1] \n",
    "    #           for src_idx in contour['sources']):\n",
    "   #         gc_contour = contour['index']\n",
    "   #         break\n",
    "   # print(f\"Galactic Center (0,0) is in contour: {gc_contour}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdee281",
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.io import fits\n",
    "from astropy.wcs import WCS\n",
    "\n",
    "# Load FITS file\n",
    "with fits.open('nH_analysis/column_density_inner20deg_filled.fits') as hdul:\n",
    "    data_nH = hdul[0].data\n",
    "    header_nH = hdul[0].header\n",
    "    wcs_nH = WCS(header_nH)\n",
    "\n",
    "with fits.open('extinction_analysis/VVVextmap_mef_ciao_ready.fits') as hdul:\n",
    "    data_ext = hdul[0].data\n",
    "    header_ext = hdul[0].header\n",
    "    wcs_ext = WCS(header_ext)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd45ef29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine groups based on extinction and nH\n",
    "import numpy as np\n",
    "from astropy.io import fits\n",
    "count = 0\n",
    "\n",
    "def cross_reference_tables(extinction_fits, nh_fits,count):\n",
    "    id_list = xmmDR14_id[y]\n",
    "    \"\"\"\n",
    "    Optimized cross-referencing of two FITS tables with contour assignments\n",
    "    Returns groups of sources in matching contour regions\n",
    "    \"\"\"\n",
    "    # Load extinction table\n",
    "    with fits.open(extinction_fits) as hdul:\n",
    "        ext_data = hdul[1].data\n",
    "        ext_contours = ext_data['contour_index']\n",
    "        ext_src_id = ext_data['SrcID']\n",
    "    \n",
    "    # Load nH table and create lookup dictionary\n",
    "    with fits.open(nh_fits) as hdul:\n",
    "        nh_data = hdul[1].data\n",
    "        nh_contour_map = {row['SrcID']: row['contour_index'] for row in nh_data}\n",
    "    \n",
    "    # Create dictionary to store results\n",
    "    contour_groups = {}\n",
    "    \n",
    "    # Single pass through extinction data\n",
    "    for i, src_id in enumerate(ext_src_id):\n",
    "        ext_contour = ext_contours[i]\n",
    "        \n",
    "        # Skip unassigned sources\n",
    "        if ext_contour == -1:\n",
    "            continue\n",
    "        \n",
    "        # Get nH contour from lookup dictionary\n",
    "        nh_contour = nh_contour_map.get(src_id, -1)\n",
    "        if nh_contour == -1:\n",
    "            continue\n",
    "        \n",
    "        # Create group key\n",
    "        key = (ext_contour, nh_contour)\n",
    "        \n",
    "        # Initialize group if needed\n",
    "        if key not in contour_groups:\n",
    "            contour_groups[key] = []\n",
    "            count=count+1\n",
    "            \n",
    "        \n",
    "        # Add source to group\n",
    "        row_index = np.where(id_list == src_id)[0]\n",
    "        row_index=row_index[0]\n",
    "        l1 = l_filt[row_index]\n",
    "        b1 = b_filt[row_index]\n",
    "        x_pix1, y_pix1 = wcs_nH.wcs_world2pix(l1, b1, 0)\n",
    "        x_pix1 = int(round(float(x_pix1)))\n",
    "        y_pix1 = int(round(float(y_pix1)))\n",
    "        x_pix2, y_pix2 = wcs_ext.wcs_world2pix(l1, b1, 0)\n",
    "        x_pix2 = int(round(float(x_pix2)))\n",
    "        y_pix2 = int(round(float(y_pix2)))\n",
    "        value1 = np.float64(data_nH[y_pix1, x_pix1])\n",
    "        value2 = data_ext[y_pix2, x_pix2]\n",
    "        \n",
    "        contour_groups[key].append({\n",
    "            'src_id': src_id,\n",
    "            'extinction_index': value2,\n",
    "            'nh_index': np.float64(value1)\n",
    "        })\n",
    "    \n",
    "    return contour_groups, count\n",
    "\n",
    "# Configuration\n",
    "extinction_table = \"extinction_analysis/filtered_XMM_DR14_with_contour.fits\"\n",
    "nh_table = \"nH_analysis/filtered_XMM_DR14_with_contour_test.fits\"\n",
    "\n",
    "# Perform cross-referencing\n",
    "result,count = cross_reference_tables(extinction_table, nh_table,count)\n",
    "\n",
    "# Print results\n",
    "print(f\"Found {len(result)} contour groups:\")\n",
    "for (ext_cont, nh_cont), sources in result.items():\n",
    "    print(f\"Extinction Contour {ext_cont} + nH Contour {nh_cont}:\")\n",
    "    print(f\"  Contains {len(sources)} sources\")\n",
    "    if sources:\n",
    "        print(f\"  Example source ID: {sources[0]['src_id']}\\n\")\n",
    "print(count)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8367c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#hardness grouping, needs result\n",
    "import math\n",
    "print(len(result))\n",
    "hardness_labels = [\"-1\",\"-0.5\",\"0\",\"0.5\",\"1\"]\n",
    "print(hardness_labels)\n",
    "count=0\n",
    "hardness = xmmDR14_EPhr4[y]\n",
    "id_list = xmmDR14_id[y]\n",
    "hardness_dict = {}\n",
    "for i, (key, contour) in enumerate(result.items()):\n",
    "    sources_in_group = result.get(key, [])\n",
    "    hardness_dict[key] = {}\n",
    "    for source_dict in sources_in_group:\n",
    "        \n",
    "        source_id = source_dict['src_id']\n",
    "        row_index = np.where(id_list == source_id)[0]\n",
    "        row_index=row_index[0]\n",
    "        if hardness[row_index] == 1:\n",
    "            if \"1+\" not in hardness_dict[key]:\n",
    "                hardness_dict[key][\"1+\"] = []\n",
    "                count=count+1\n",
    "                \n",
    "            hardness_dict[key][\"1+\"].append({\n",
    "                    'src_id': source_id,\n",
    "                    'hardness_ratio': hardness[row_index]\n",
    "                })\n",
    "        elif hardness[row_index] == 0:\n",
    "            if \"0_exact\" not in hardness_dict[key]:\n",
    "                hardness_dict[key][\"0_exact\"] = []\n",
    "                count=count+1\n",
    "                \n",
    "            hardness_dict[key][\"0_exact\"].append({\n",
    "                    'src_id': source_id,\n",
    "                    'hardness_ratio': hardness[row_index]\n",
    "                })\n",
    "        elif math.isnan(hardness[row_index]):\n",
    "            if \"nan\" not in hardness_dict[key]:\n",
    "                hardness_dict[key][\"nan\"] = []\n",
    "                count=count+1\n",
    "            hardness_dict[key][\"nan\"].append({\n",
    "                    'src_id': source_id,\n",
    "                    'hardness_ratio': hardness[row_index]\n",
    "                }) \n",
    "        else:\n",
    "            for label in hardness_labels:\n",
    "                if hardness[row_index] <= float(label):\n",
    "                    if label not in hardness_dict[key]:\n",
    "                        hardness_dict[key][label] = []\n",
    "                        count=count+1\n",
    "                    \n",
    "                    hardness_dict[key][label].append({\n",
    "                        'src_id': source_id,\n",
    "                        'hardness_ratio': hardness[row_index]\n",
    "                    })\n",
    "                    break\n",
    "                \n",
    "print(\"Done\")\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3f2f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#flux grouping, needs result\n",
    "import math\n",
    "count = 0\n",
    "print(len(result))\n",
    "flux_labels = [\"1e-16\",\"1e-14\",\"1e-13\",\"1e-11\"]\n",
    "print(flux_labels)\n",
    "flux = fx2to12\n",
    "id_list = xmmDR14_id[y]\n",
    "flux_dict = {}\n",
    "for i, (key, contour) in enumerate(result.items()):\n",
    "    sources_in_group = result.get(key, [])\n",
    "    flux_dict[key] = {}\n",
    "    for source_dict in sources_in_group:\n",
    "        \n",
    "        source_id = source_dict['src_id']\n",
    "        row_index = np.where(id_list == source_id)[0]\n",
    "        row_index=row_index[0]\n",
    "        \n",
    "        if math.isnan(flux[row_index]):\n",
    "            if \"nan\" not in flux_dict[key]:\n",
    "                flux_dict[key][\"nan\"] = []\n",
    "                count = count+1\n",
    "            flux_dict[key][\"nan\"].append({\n",
    "                    'src_id': source_id,\n",
    "                    'flux': flux[row_index]\n",
    "                }) \n",
    "        else:\n",
    "            for label in flux_labels:\n",
    "                if flux[row_index] <= float(label):\n",
    "                    if label not in flux_dict[key]:\n",
    "                        flux_dict[key][label] = []\n",
    "                        count = count+1\n",
    "                    \n",
    "                    flux_dict[key][label].append({\n",
    "                        'src_id': source_id,\n",
    "                        'nh': source_dict['nh_index'],\n",
    "                        'extinction': source_dict['extinction_index'],\n",
    "                        'flux': flux[row_index]\n",
    "                    })\n",
    "                    break\n",
    "                \n",
    "print(\"Done\")\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c3b39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine groups based on flux and hardness\n",
    "import numpy as np\n",
    "from astropy.io import fits\n",
    "count = 0\n",
    "\n",
    "def find_all_paths_to_value(d, target_value, path=None):\n",
    "    if path is None:\n",
    "        path = []\n",
    "    paths = []\n",
    "    if isinstance(d, dict):\n",
    "        for k, v in d.items():\n",
    "            new_path = path + [k]\n",
    "            if isinstance(v, (dict, list)):\n",
    "                paths.extend(find_all_paths_to_value(v, target_value, new_path))\n",
    "            else:\n",
    "                if v == target_value:\n",
    "                    paths.append(new_path)\n",
    "    elif isinstance(d, list):\n",
    "        for idx, v in enumerate(d):\n",
    "            new_path = path + [idx]\n",
    "            if isinstance(v, (dict, list)):\n",
    "                paths.extend(find_all_paths_to_value(v, target_value, new_path))\n",
    "            else:\n",
    "                if v == target_value:\n",
    "                    paths.append(new_path)\n",
    "    return paths\n",
    "\n",
    "def cross_reference_tables(flux_dict, hardness_dict,count):\n",
    "\n",
    "    # Create dictionary to store results\n",
    "    combined_groups = {}\n",
    "    \n",
    "    for i, (key, inner_dict) in enumerate(flux_dict.items()):\n",
    "        combined_groups[key] = {}\n",
    "        for i, (inner_key, sources) in enumerate(inner_dict.items()):\n",
    "            flux_key = inner_key\n",
    "            for source in sources:\n",
    "                flux = source.get('flux')\n",
    "                source_id = source.get('src_id')\n",
    "                hardness_keys = find_all_paths_to_value(hardness_dict[key],source_id)\n",
    "                hardness_path = hardness_keys[0]\n",
    "                hardness = hardness_dict[key][hardness_path[0]][0]['hardness_ratio']\n",
    "                if math.isnan(hardness):\n",
    "                    continue\n",
    "            \n",
    "                inner_key = (flux_key,hardness_path[0])\n",
    "                if inner_key not in combined_groups[key]:\n",
    "                    combined_groups[key][inner_key] = []\n",
    "                    count = count+1\n",
    "                    \n",
    "            \n",
    "                combined_groups[key][inner_key].append({\n",
    "                    'src_id': source_id,\n",
    "                    'nh': source['nh'],\n",
    "                    'extinction': source['extinction'],\n",
    "                    'flux': flux,\n",
    "                    'hardness': hardness\n",
    "                })\n",
    "                \n",
    "                \n",
    "        \n",
    "    \n",
    "    return combined_groups, count\n",
    "\n",
    "# Configuration\n",
    "\n",
    "# Perform cross-referencing\n",
    "combined_dict,count = cross_reference_tables(flux_dict, hardness_dict,count)\n",
    "\n",
    "# Print results\n",
    "print(f\"Found {len(combined_dict)} contour groups:\")\n",
    "#for (ext_cont, nh_cont), sources in result.items():\n",
    "#    print(f\"Extinction Contour {ext_cont} + nH Contour {nh_cont}:\")\n",
    "#    print(f\"  Contains {len(sources)} sources\")\n",
    "#    if sources:\n",
    "#        print(f\"  Example source ID: {sources[0]['src_id']}\\n\")\n",
    "print(count)\n",
    "combined_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e94105",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_small_source_groups(data,count):\n",
    "    \n",
    "    \"\"\"\n",
    "    Merge all single-source groups that are within a certain galactic longitude/latitude\n",
    "    and whose keys are within a certain error range.\n",
    "\n",
    "    Parameters:\n",
    "    - data: dict of dicts, each level keyed by a tuple, innermost value is a list of sources\n",
    "    - l_idx, b_idx: indices in the tuple keys for longitude and latitude\n",
    "    - l_tol, b_tol: max allowed difference in longitude and latitude for merging\n",
    "    - key_error_tols: dict mapping index to allowed error for other tuple elements (e.g., {2: 0.05})\n",
    "\n",
    "    Returns:\n",
    "    - merged_groups: list of lists, each list is a merged group of sources\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "\n",
    "    # Flatten all single-source groups with their keys\n",
    "    small_groups = []\n",
    "    for outer_key, inner_dict in data.items():\n",
    "        for inner_key, sources in inner_dict.items():\n",
    "            if isinstance(sources, list) and len(sources) <= 5:\n",
    "                for k,source in enumerate(sources):\n",
    "                    small_groups.append({\n",
    "                        'outer_key': outer_key,\n",
    "                        'inner_key': inner_key,\n",
    "                        'source': source\n",
    "                    })\n",
    "\n",
    "    merged = []\n",
    "    used = set()\n",
    "\n",
    "    for i, entry in enumerate(small_groups):\n",
    "        src_id = entry['source']['src_id']\n",
    "        if src_id in used:\n",
    "            continue\n",
    "        group = [entry]\n",
    "        used.add(src_id)\n",
    "        row_index = np.where(id_list == entry['source']['src_id'])[0]\n",
    "        row_index=row_index[0]\n",
    "        l1 = l_filt[row_index]\n",
    "        b1 = b_filt[row_index]\n",
    "        key1 = entry['outer_key']\n",
    "        inner1 = entry['inner_key']\n",
    "\n",
    "        for j, other in enumerate(small_groups):\n",
    "            other_src_id = other['source']['src_id']\n",
    "            if j == i or other_src_id in used:\n",
    "                continue\n",
    "            row_index2 = np.where(id_list == other['source']['src_id'])[0]\n",
    "            row_index2 = row_index2[0]\n",
    "            l2 = l_filt[row_index2]\n",
    "            b2 = b_filt[row_index2]\n",
    "            key2 = other['outer_key']\n",
    "            inner2 = other['inner_key']\n",
    "            \n",
    "            # Check galactic coordinate proximity\n",
    "            if abs(l1 - l2) > 1 or abs(b1 - b2) > 1: \n",
    "                continue\n",
    "            # Check all other key error tolerances\n",
    "            if abs(np.log10(entry['source']['nh'])-np.log10(other['source']['nh'])) > 0.5:\n",
    "                \n",
    "                continue\n",
    "            \n",
    "            if abs(entry['source']['extinction']-other['source']['extinction']) > 7:\n",
    "                \n",
    "                continue\n",
    "               \n",
    "            if abs(np.log10(entry['source']['flux'])-np.log10(other['source']['flux'])) > 1.7:\n",
    "                \n",
    "                continue\n",
    "            \n",
    "            if abs(entry['source']['hardness']-other['source']['hardness']) > 0.5:\n",
    "                \n",
    "                continue\n",
    "            \n",
    "            group.append(other)\n",
    "            used.add(other_src_id)\n",
    "            count=count+1\n",
    "\n",
    "        merged.append(group)\n",
    "    \n",
    "    count1=0\n",
    "    for groups in merged:\n",
    "        count1=count1+len(groups)\n",
    "    print(count1)\n",
    "    merged_dict = {}\n",
    "    print(len(used))\n",
    "    for group in merged:\n",
    "        # Gather all unique outer and inner keys from the group\n",
    "        outer_keys = tuple(sorted(set(entry['outer_key'] for entry in group)))\n",
    "        inner_keys = tuple(sorted(set(entry['inner_key'] for entry in group)))\n",
    "        # Gather all sources in this group\n",
    "        sources = [entry['source'] for entry in group]\n",
    "\n",
    "        if outer_keys not in merged_dict:\n",
    "            merged_dict[outer_keys] = {}\n",
    "        if inner_keys not in merged_dict[outer_keys]:\n",
    "            merged_dict[outer_keys][inner_keys] = []\n",
    "        merged_dict[outer_keys][inner_keys].extend(sources)\n",
    "\n",
    "    return merged_dict,count\n",
    "count=0\n",
    "merged_group_small_test,count = merge_small_source_groups(combined_dict,count)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fef528",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove small groups and merge with resorted/combined small groups\n",
    "def remove_small_source_groups(data):\n",
    "    data_copy = {}\n",
    "    for outer_key, inner_dict in data.items():\n",
    "        new_inner = {}\n",
    "        for inner_key, sources in inner_dict.items():\n",
    "            if not (isinstance(sources, list) and len(sources) <= 5):\n",
    "                new_inner[inner_key] = sources\n",
    "        if new_inner:\n",
    "            data_copy[outer_key] = new_inner\n",
    "    return data_copy\n",
    "\n",
    "combined_dict_copy = remove_small_source_groups(combined_dict)\n",
    "\n",
    "def merge_dictionaries(dict1, dict2):\n",
    "    \"\"\"\n",
    "    Merge two two-layer dictionaries. dict2 entries overwrite dict1 where keys overlap.\n",
    "    \"\"\"\n",
    "    result = dict1.copy()\n",
    "    for outer_key, inner_dict in dict2.items():\n",
    "        if outer_key not in result:\n",
    "            result[outer_key] = {}\n",
    "        for inner_key, sources in inner_dict.items():\n",
    "            result[outer_key][inner_key] = sources\n",
    "    return result\n",
    "final_test = merge_dictionaries(combined_dict_copy,merged_group_small_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6498045f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dist sig clipping\n",
    "from astropy.stats import sigma_clip\n",
    "import numpy as np\n",
    "combined_dict_test_dist = final_test.copy()\n",
    "count1 = 1\n",
    "count3 = 0\n",
    "count_list = []\n",
    "while count1 > 0 and count3 < 1000:\n",
    "    all_meanl = []\n",
    "    all_meanb = []\n",
    "    outliers = 0\n",
    "    clipped_dict = {}\n",
    "    remain_dict = {}\n",
    "    count1=0\n",
    "    count2=0\n",
    "    for i, (key, inner_dict) in enumerate(combined_dict_test_dist.items()): \n",
    "        if key not in clipped_dict:\n",
    "            clipped_dict[key] = {}\n",
    "        if key not in remain_dict:\n",
    "            remain_dict[key] = {}\n",
    "        for j, (inner_key, sources) in enumerate(inner_dict.items()):\n",
    "            if inner_key not in clipped_dict[key]:\n",
    "                clipped_dict[key][inner_key] = []\n",
    "            if inner_key not in remain_dict[key] or len(sources) != 1:\n",
    "                remain_dict[key][inner_key] = {}\n",
    "                remain_dict[key][inner_key]['sources'] = []\n",
    "\n",
    "            meanl = []\n",
    "            meanb = []\n",
    "            if count3 == 0:\n",
    "                for source in sources:\n",
    "                    row_index = np.where(id_list == source['src_id'])[0]\n",
    "                    row_index=row_index[0]\n",
    "                    meanl.append(l_filt[row_index])\n",
    "                    meanb.append(b_filt[row_index])\n",
    "            else:\n",
    "                for source in sources['sources']:\n",
    "                    row_index = np.where(id_list == source['src_id'])[0]\n",
    "                    row_index=row_index[0]\n",
    "                    meanl.append(l_filt[row_index])\n",
    "                    meanb.append(b_filt[row_index])\n",
    "            meanl = np.array(meanl, dtype=np.float64)\n",
    "            meanb = np.array(meanb, dtype=np.float64)\n",
    "            all_meanl.append(np.mean(meanl))\n",
    "            all_meanb.append(np.mean(meanb))\n",
    "            # Perform sigma clipping (e.g., 3-sigma)\n",
    "\n",
    "\n",
    "            mask_l = sigma_clip(all_meanl, sigma=2, maxiters=5).mask\n",
    "            mask_b = sigma_clip(all_meanb, sigma=2, maxiters=5).mask\n",
    "\n",
    "            joint_mask = mask_l | mask_b  # True if outlier in l or b\n",
    "            \n",
    "            outlier_indices = np.where(joint_mask)[0]\n",
    "            outliers = outliers + len(outlier_indices)\n",
    "            if count3 == 0:\n",
    "                for i, source in enumerate(sources):\n",
    "                    if i in outlier_indices:\n",
    "                        count1 = count1+1\n",
    "                    if len(sources) == 1:\n",
    "                        count2 = count2+1\n",
    "                    if i in outlier_indices or len(sources) == 1:\n",
    "                        clipped_dict[key][inner_key].append({\n",
    "                            'src_id': source['src_id'],\n",
    "                            'nh': source['nh'],\n",
    "                            'extinction': source['extinction'],\n",
    "                            'flux': source['flux'],\n",
    "                            'hardness': source['hardness']\n",
    "                        })\n",
    "                    else:\n",
    "                        remain_dict[key][inner_key]['sources'].append({\n",
    "                            'src_id': source['src_id'],\n",
    "                            'nh': source['nh'],\n",
    "                            'extinction': source['extinction'],\n",
    "                            'flux': source['flux'],\n",
    "                            'hardness': source['hardness']\n",
    "                        })\n",
    "            else:\n",
    "                for i, source in enumerate(sources['sources']):\n",
    "                    if i in outlier_indices:\n",
    "                        count1 = count1+1\n",
    "                    if len(sources) == 1:\n",
    "                        count2 = count2+1\n",
    "                    if i in outlier_indices or len(sources) == 1:\n",
    "                        clipped_dict[key][inner_key].append({\n",
    "                            'src_id': source['src_id'],\n",
    "                            'nh': source['nh'],\n",
    "                            'extinction': source['extinction'],\n",
    "                            'flux': source['flux'],\n",
    "                            'hardness': source['hardness']\n",
    "                        })\n",
    "                    else:\n",
    "                        remain_dict[key][inner_key]['sources'].append({\n",
    "                            'src_id': source['src_id'],\n",
    "                            'nh': source['nh'],\n",
    "                            'extinction': source['extinction'],\n",
    "                            'flux': source['flux'],\n",
    "                            'hardness': source['hardness']\n",
    "                        })\n",
    "            l_list = []\n",
    "            b_list = []\n",
    "            hardness_list = []\n",
    "            flux_list = []\n",
    "            nH_list = []\n",
    "            extinction_list = []\n",
    "            for source in remain_dict[key][inner_key]['sources']:\n",
    "                if len(remain_dict[key][inner_key]['sources']) != 0: \n",
    "                    nH_list.append(source['nh'])\n",
    "                    extinction_list.append(source['extinction'])\n",
    "                    flux_list.append(source['flux'])\n",
    "                    hardness_list.append(source['hardness'])\n",
    "                    row_index = np.where(id_list == source['src_id'])[0]\n",
    "                    row_index=row_index[0]\n",
    "                    l_list.append(l_filt[row_index])\n",
    "                    b_list.append(b_filt[row_index])\n",
    "            if len(remain_dict[key][inner_key]['sources']) != 0: \n",
    "                remain_dict[key][inner_key]['meta']=[]\n",
    "                remain_dict[key][inner_key]['meta'].append({\n",
    "                    'mean_l': np.mean(l_list),\n",
    "                    'mean_b': np.mean(b_list),\n",
    "                    'mean_hardness': np.mean(hardness_list),\n",
    "                    'mean_flux': np.mean(flux_list),\n",
    "                    'mean_nH': np.mean(nH_list),\n",
    "                    'mean_extinction': np.mean(extinction_list)\n",
    "                })\n",
    "\n",
    "    print(count1,count2)\n",
    "    combined_dict_test_dist = {}\n",
    "    count=0\n",
    "    for i, (key, inner_dict) in enumerate(clipped_dict.items()):\n",
    "        for j, (inner_key, sources) in enumerate(inner_dict.items()):\n",
    "            if len(sources) != 0:\n",
    "\n",
    "                for k, source in enumerate(sources):\n",
    "                    flux_minl = 1000\n",
    "                    flux_minb = 1000\n",
    "                    flux_key = ['','']\n",
    "                    row_index = np.where(id_list == source['src_id'])[0]\n",
    "                    row_index=row_index[0]\n",
    "                    l = l_filt[row_index]\n",
    "                    b = b_filt[row_index]\n",
    "                    for mean_key, mean_inner_dict in remain_dict.items():\n",
    "                        if mean_key not in combined_dict_test_dist:\n",
    "                            combined_dict_test_dist[mean_key] = {}\n",
    "                        for mean_inner_key, mean_sources in mean_inner_dict.items():\n",
    "                            if len(mean_sources['sources']) != 0:\n",
    "                                if mean_inner_key not in combined_dict_test_dist[mean_key]:\n",
    "                                    combined_dict_test_dist[mean_key][mean_inner_key] = {}\n",
    "                                    combined_dict_test_dist[mean_key][mean_inner_key]['sources'] = mean_sources['sources'].copy()\n",
    "                                    combined_dict_test_dist[mean_key][mean_inner_key]['meta'] = mean_sources['meta'].copy()\n",
    "                                mean_list = mean_sources['meta']\n",
    "                                mean_list = mean_list[0]\n",
    "\n",
    "                                if abs(l-mean_list['mean_l']) < flux_minl and abs(b-mean_list['mean_b']) < flux_minb:\n",
    "                                    flux_minl = abs(l-mean_list['mean_l'])\n",
    "                                    flux_minb = abs(b-mean_list['mean_b'])\n",
    "                                    flux_key = [mean_key,mean_inner_key]\n",
    "                    print(flux_key)\n",
    "                    mean_list = remain_dict[flux_key[0]][flux_key[1]]['meta'][0]\n",
    "                    \n",
    "                    if abs(source['hardness']-mean_list['mean_hardness']) < 0.5 and abs(np.log10(source['flux'])-np.log10(mean_list['mean_flux'])) < 1.7 and abs(source['extinction']-mean_list['mean_extinction']) < 7 and abs(np.log10(np.float64(source['nh']))-np.log10(np.float64(mean_list['mean_nH']))) < 0.5:\n",
    "                        if source not in combined_dict_test_dist[flux_key[0]][flux_key[1]]['sources']:\n",
    "                            combined_dict_test_dist[flux_key[0]][flux_key[1]]['sources'].append(source)\n",
    "                    else:\n",
    "                        if key not in combined_dict_test_dist:\n",
    "                            combined_dict_test_dist[key] = {}\n",
    "                        if inner_key not in combined_dict_test_dist[key]:\n",
    "                            combined_dict_test_dist[key][inner_key] = {}\n",
    "                            combined_dict_test_dist[key][inner_key]['sources'] = []\n",
    "\n",
    "                        combined_dict_test_dist[key][inner_key]['sources'].append(source)\n",
    "    print(count) \n",
    "    count_list.append(count1)\n",
    "    #if count3 > 0:\n",
    "       # if count_list[count3-1] == count_list[count3]:\n",
    "       #     break\n",
    "    count3 = count3+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192b63ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sig clipping\n",
    "from astropy.stats import sigma_clip\n",
    "import numpy as np\n",
    "loop=['nh','flux','extinction','hardness']\n",
    "loop_num = 2\n",
    "count4 = 0\n",
    "clip_list=[]\n",
    "count5=0\n",
    "for x in range(loop_num):\n",
    "    #print(count5)\n",
    "    for clip_type in loop:\n",
    "        print(count4)\n",
    "        if count4 == 0:\n",
    "            #print('flag')\n",
    "            #print(clip_type)\n",
    "            combined_dict_test = combined_dict_test_dist.copy()\n",
    "        else:\n",
    "            combined_dict_test = combined_dict_test\n",
    "        count1 = 1\n",
    "        count3 = 0\n",
    "        count_list = []\n",
    "        while count1 > 0 and count3 < 1000:\n",
    "            all_mean = []\n",
    "            outliers = 0\n",
    "            clipped_dict = {}\n",
    "            remain_dict = {}\n",
    "            count1=0\n",
    "            count2=0\n",
    "            for i, (key, inner_dict) in enumerate(combined_dict_test.items()): \n",
    "                if key not in clipped_dict:\n",
    "                    clipped_dict[key] = {}\n",
    "                if key not in remain_dict:\n",
    "                    remain_dict[key] = {}\n",
    "                for j, (inner_key, sources) in enumerate(inner_dict.items()):\n",
    "                    if inner_key not in clipped_dict[key]:\n",
    "                        clipped_dict[key][inner_key] = []\n",
    "                    if inner_key not in remain_dict[key] or len(sources) != 1:\n",
    "                        remain_dict[key][inner_key] = {}\n",
    "                        remain_dict[key][inner_key]['sources'] = []\n",
    "\n",
    "                    mean = []\n",
    "                    #if count3 == 0 and count4 == 0:\n",
    "                    #    for source in sources:\n",
    "                            \n",
    "                    #        mean.append(source[clip_type])\n",
    "                    #else:\n",
    "                    for source in sources['sources']:\n",
    "                        mean.append(source[clip_type])\n",
    "                    mean = np.array(mean, dtype=np.float64)\n",
    "                    all_mean.append(np.mean(mean))\n",
    "                    # Perform sigma clipping (e.g., 3-sigma)\n",
    "                    clipped = sigma_clip(mean, sigma=2, maxiters=5)\n",
    "                    # Boolean mask: True for values that are clipped (outliers)\n",
    "                    outliers_mask = clipped.mask\n",
    "                    # Indices of outliers\n",
    "                    outlier_indices = np.where(outliers_mask)[0]\n",
    "                    outliers = outliers + len(outlier_indices)\n",
    "                    '''if count3 ==0 and count4 == 0:\n",
    "                        for i, source in enumerate(sources):\n",
    "                            if i in outlier_indices:\n",
    "                                count1 = count1+1\n",
    "                            if len(sources) == 1:\n",
    "                                count2 = count2+1\n",
    "                            if i in outlier_indices or len(sources) == 1:\n",
    "                                clipped_dict[key][inner_key].append({\n",
    "                                    'src_id': source['src_id'],\n",
    "                                    'nh': source['nh'],\n",
    "                                    'extinction': source['extinction'],\n",
    "                                    'flux': source['flux'],\n",
    "                                    'hardness': source['hardness']\n",
    "                                })\n",
    "                            else:\n",
    "                                remain_dict[key][inner_key]['sources'].append({\n",
    "                                    'src_id': source['src_id'],\n",
    "                                    'nh': source['nh'],\n",
    "                                    'extinction': source['extinction'],\n",
    "                                    'flux': source['flux'],\n",
    "                                    'hardness': source['hardness']\n",
    "                                })\n",
    "                    else:'''\n",
    "                    for i, source in enumerate(sources['sources']):\n",
    "                        if i in outlier_indices:\n",
    "                            count1 = count1+1\n",
    "                        if len(sources) == 1:\n",
    "                            count2 = count2+1\n",
    "                        if i in outlier_indices or len(sources) == 1:\n",
    "                            clipped_dict[key][inner_key].append({\n",
    "                                'src_id': source['src_id'],\n",
    "                                'nh': source['nh'],\n",
    "                                'extinction': source['extinction'],\n",
    "                                'flux': source['flux'],\n",
    "                                'hardness': source['hardness']\n",
    "                            })\n",
    "                        else:\n",
    "                            remain_dict[key][inner_key]['sources'].append({\n",
    "                                'src_id': source['src_id'],\n",
    "                                'nh': source['nh'],\n",
    "                                'extinction': source['extinction'],\n",
    "                                'flux': source['flux'],\n",
    "                                'hardness': source['hardness']\n",
    "                            })\n",
    "                    l_list = []\n",
    "                    b_list = []\n",
    "                    hardness_list = []\n",
    "                    flux_list = []\n",
    "                    nH_list = []\n",
    "                    extinction_list = []\n",
    "                    for source in remain_dict[key][inner_key]['sources']:\n",
    "                        if len(remain_dict[key][inner_key]['sources']) != 0: \n",
    "                            nH_list.append(source['nh'])\n",
    "                            extinction_list.append(source['extinction'])\n",
    "                            flux_list.append(source['flux'])\n",
    "                            hardness_list.append(source['hardness'])\n",
    "                            row_index = np.where(id_list == source['src_id'])[0]\n",
    "                            row_index=row_index[0]\n",
    "                            l_list.append(l_filt[row_index])\n",
    "                            b_list.append(b_filt[row_index])\n",
    "                    if len(remain_dict[key][inner_key]['sources']) != 0: \n",
    "                        remain_dict[key][inner_key]['meta']=[]\n",
    "                        remain_dict[key][inner_key]['meta'].append({\n",
    "                            'mean_l': np.mean(l_list),\n",
    "                            'mean_b': np.mean(b_list),\n",
    "                            'mean_hardness': np.mean(hardness_list),\n",
    "                            'mean_flux': np.mean(flux_list),\n",
    "                            'mean_nH': np.mean(nH_list),\n",
    "                            'mean_extinction': np.mean(extinction_list)\n",
    "                        })\n",
    "\n",
    "            #print(count1,count2)\n",
    "            combined_dict_test = {}\n",
    "            count=0\n",
    "            for i, (key, inner_dict) in enumerate(clipped_dict.items()):\n",
    "                for j, (inner_key, sources) in enumerate(inner_dict.items()):\n",
    "                    if len(sources) != 0:\n",
    "                        for k, source in enumerate(sources):\n",
    "                            flux_min = 1000\n",
    "                            flux_key = ['','']\n",
    "                            row_index = np.where(id_list == source['src_id'])[0]\n",
    "                            row_index=row_index[0]\n",
    "                            l = l_filt[row_index]\n",
    "                            b = b_filt[row_index]\n",
    "                            check = False\n",
    "                            for mean_key, mean_inner_dict in remain_dict.items():\n",
    "                                if mean_key not in combined_dict_test:\n",
    "                                    combined_dict_test[mean_key] = {}\n",
    "                                for mean_inner_key, mean_sources in mean_inner_dict.items():\n",
    "                                    if len(mean_sources['sources']) != 0:\n",
    "                                        if mean_inner_key not in combined_dict_test[mean_key]:\n",
    "                                            combined_dict_test[mean_key][mean_inner_key] = {}\n",
    "                                            combined_dict_test[mean_key][mean_inner_key]['sources'] = mean_sources['sources'].copy()\n",
    "                                            combined_dict_test[mean_key][mean_inner_key]['meta'] = mean_sources['meta'].copy()\n",
    "                                        mean_list = mean_sources['meta']\n",
    "                                        mean_list = mean_list[0]\n",
    "                                        boolean_expr = 0\n",
    "                                        if clip_type == 'nh':\n",
    "                                            boolean_expr = abs(np.log10(np.float64(source['nh']))-np.log10(np.float64(mean_list['mean_nH'])))\n",
    "                                        elif clip_type == 'flux':\n",
    "                                            boolean_expr = abs(np.log10(np.float64(source['flux']))-np.log10(np.float64(mean_list['mean_flux'])))\n",
    "                                        elif clip_type == 'extinction':\n",
    "                                            boolean_expr = abs(source['extinction']-mean_list['mean_extinction'])\n",
    "                                        else:\n",
    "                                            boolean_expr = abs(source['hardness']-mean_list['mean_hardness'])\n",
    "                                        \n",
    "                                        if boolean_expr < flux_min and abs(l-mean_list['mean_l']) < 1.5 and abs(b-mean_list['mean_b']) < 1.5:\n",
    "                                            flux_min = boolean_expr\n",
    "                                            flux_key = [mean_key,mean_inner_key]\n",
    "                                            check=True \n",
    "                            if not check:\n",
    "                                print('flag')\n",
    "                                #print(count4)\n",
    "                                        \n",
    "                            #print(flux_key)\n",
    "                            mean_list = remain_dict[flux_key[0]][flux_key[1]]['meta'][0]\n",
    "                            boolean = False\n",
    "                            if clip_type == 'nh':\n",
    "                                boolean =  (abs(source['hardness']-mean_list['mean_hardness']) < 0.5 and abs(np.log10(source['flux'])-np.log10(mean_list['mean_flux'])) < 1.7 and abs(source['extinction']-mean_list['mean_extinction']) < 7)\n",
    "                            elif clip_type == 'flux':\n",
    "                                boolean = (abs(source['hardness']-mean_list['mean_hardness']) < 0.5 and abs(np.log10(np.float64(source['nh']))-np.log10(np.float64(mean_list['mean_nH']))) < 0.5 and abs(source['extinction']-mean_list['mean_extinction']) < 7)\n",
    "                            elif clip_type == 'extinction':\n",
    "                                boolean = (abs(source['hardness']-mean_list['mean_hardness']) < 0.5 and abs(np.log10(np.float64(source['nh']))-np.log10(np.float64(mean_list['mean_nH']))) < 0.5 and abs(np.log10(source['flux'])-np.log10(mean_list['mean_flux'])) < 1.7)\n",
    "                            else:\n",
    "                                boolean_expr = (abs(source['extinction']-mean_list['mean_extinction']) < 7 and abs(np.log10(np.float64(source['nh']))-np.log10(np.float64(mean_list['mean_nH']))) < 0.5 and abs(np.log10(source['flux'])-np.log10(mean_list['mean_flux'])) < 1.7)\n",
    "                            if boolean_expr:\n",
    "                                if source not in combined_dict_test[flux_key[0]][flux_key[1]]['sources']:\n",
    "                                    combined_dict_test[flux_key[0]][flux_key[1]]['sources'].append(source)\n",
    "                            else:\n",
    "                                if key not in combined_dict_test:\n",
    "                                    combined_dict_test[key] = {}\n",
    "                                if inner_key not in combined_dict_test[key]:\n",
    "                                    combined_dict_test[key][inner_key] = {}\n",
    "                                    combined_dict_test[key][inner_key]['sources'] = []\n",
    "\n",
    "                                combined_dict_test[key][inner_key]['sources'].append(source)     \n",
    "            #print(count1) \n",
    "            count_list.append(count1)\n",
    "            if count3 > 0:\n",
    "                if count_list[count3-1] == count_list[count3]:\n",
    "                    break\n",
    "            count3 = count3+1\n",
    "            \n",
    "        count4 = count4+1\n",
    "        for key, inner_dict in combined_dict_test.items():\n",
    "            for inner_key, sources in inner_dict.items():\n",
    "                if len(combined_dict_test[key][inner_key]['sources']) > 1:\n",
    "                    l_list = []\n",
    "                    b_list = []\n",
    "                    hardness_list = []\n",
    "                    flux_list = []\n",
    "                    nH_list = []\n",
    "                    extinction_list = []\n",
    "                    combined_dict_test[key][inner_key]['meta'] = []\n",
    "                    for source in sources['sources']:\n",
    "                        if len(combined_dict_test[key][inner_key]['sources']) != 0: \n",
    "                            nH_list.append(source['nh'])\n",
    "                            extinction_list.append(source['extinction'])\n",
    "                            flux_list.append(source['flux'])\n",
    "                            hardness_list.append(source['hardness'])\n",
    "                            row_index = np.where(id_list == source['src_id'])[0]\n",
    "                            row_index=row_index[0]\n",
    "                            l_list.append(l_filt[row_index])\n",
    "                            b_list.append(b_filt[row_index])\n",
    "                    if len(combined_dict_test[key][inner_key]['sources']) != 0: \n",
    "                        combined_dict_test[key][inner_key]['meta'].append({\n",
    "                            'mean_l': np.mean(l_list),\n",
    "                            'mean_b': np.mean(b_list),\n",
    "                            'mean_hardness': np.mean(hardness_list),\n",
    "                            'mean_flux': np.mean(flux_list),\n",
    "                            'mean_nH': np.mean(nH_list),\n",
    "                            'mean_extinction': np.mean(extinction_list)\n",
    "                       })\n",
    "    clip_list.append(count1)\n",
    "    count5 = count5+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e95b269",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print information on the combined dictionary\n",
    "count_mult = 0\n",
    "length_list = []\n",
    "total_mult = 0\n",
    "combined_groups_multiple = {}\n",
    "combined_groups_one = {}\n",
    "count_one = 0\n",
    "total_one = 0\n",
    "\n",
    "for i, (key, inner_dict) in enumerate(combined_dict_test4.items()):\n",
    "        combined_groups_multiple[key] = {}\n",
    "        combined_groups_one[key] = {}\n",
    "        for i, (inner_key, sources) in enumerate(inner_dict.items()):\n",
    "            if len(sources['sources']) > 1:\n",
    "                count_mult = count_mult + 1\n",
    "                length_list.append(len(sources['sources']))\n",
    "                total_mult = total_mult + len(sources['sources'])\n",
    "                combined_groups_multiple[key][inner_key]={}\n",
    "                combined_groups_multiple[key][inner_key]['sources'] = []\n",
    "                combined_groups_multiple[key][inner_key]['meta'] = []\n",
    "                combined_groups_multiple[key][inner_key]['sources'].append(sources['sources'])\n",
    "                \n",
    "                combined_groups_multiple[key][inner_key]['meta'].append(sources['meta'])\n",
    "            else:\n",
    "                count_one = count_one + 1\n",
    "                total_one = total_one + len(sources['sources'])\n",
    "                combined_groups_one[key][inner_key]={}\n",
    "                combined_groups_one[key][inner_key]['sources'] = []\n",
    "                combined_groups_one[key][inner_key]['sources'].append(sources['sources'])\n",
    "                \n",
    "                \n",
    "print(count_one)\n",
    "print(total_one)\n",
    "print(count_mult)\n",
    "print(total_mult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e5a96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Histogram of groups above 2\n",
    "import matplotlib.pyplot as plt\n",
    "print(np.max(length_list))\n",
    "plt.rcParams['xtick.top'] = False\n",
    "data_min = np.min(length_list)\n",
    "data_max = np.max(length_list)\n",
    "print(np.min(length_list))\n",
    "n_bins = int(np.ceil(np.sqrt(len(length_list)*3.5)))\n",
    "\n",
    "# Calculate bin width\n",
    "bin_width = (data_max - data_min) / n_bins\n",
    "\n",
    "# Create bin edges\n",
    "bins = np.arange(data_min, data_max + bin_width, bin_width)\n",
    "#nbins = np.logspace(np.log10(3e-18),np.log10(np.amax(xmmDR14_sfx2to12[glist_nodupes])),28)\n",
    "#nbins = np.logspace(np.log10(np.min(nh_val)),np.log10(np.max(nh_val)),50)\n",
    "plt.figure(figsize=(8.,4.5))\n",
    "n, bins, patches  = plt.hist(length_list, color='c', edgecolor='k', bins=bins, alpha=0.75)\n",
    "# Create the histogram and get the counts and bin edges\n",
    "\n",
    "# Sum the counts for specific bins\n",
    "# For example, sum the counts of the first 3 bins\n",
    "sum_first_three_bins = np.sum(n[:4])\n",
    "plt.xlabel('Number of Sources')\n",
    "plt.ylabel('Number of Groups')\n",
    "#plt.legend(loc='upper right', labelspacing=0.3, handletextpad=0.35, borderaxespad=0.3, handlelength=1.2, frameon=False, alignment='right')#, markerfirst=False\n",
    "plt.yscale('log')\n",
    "#plt.xlim(np.min(length_list),np.max(length_list))\n",
    "plt.title('Amount of Sources in Groups')\n",
    "\n",
    "plt.ylim(1,100)\n",
    "plt.xlim(2,60)\n",
    "plt.tick_params(axis='x', which='both', direction='out')\n",
    "print(sum_first_three_bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0aaf227",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for duplicated just in case\n",
    "def count_unique_duplicates(merged_dict):\n",
    "    seen = set()\n",
    "    duplicates = set()\n",
    "    for outer_key, inner_dict in merged_dict.items():\n",
    "        for inner_key, sources in inner_dict.items():\n",
    "            for source in sources['sources']:\n",
    "                src_id = source['src_id']\n",
    "                if src_id in seen:\n",
    "                    duplicates.add(src_id)\n",
    "                else:\n",
    "                    seen.add(src_id)\n",
    "    return len(duplicates)\n",
    "\n",
    "count = count_unique_duplicates(combined_dict_test)\n",
    "count"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
