{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c093a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "combined_dict_quant = copy.deepcopy(combined_dict_test)\n",
    "for i, (key, inner_dict) in enumerate(combined_dict_quant.items()):  \n",
    "    for i, (inner_key, sources) in enumerate(inner_dict.items()):\n",
    "        id_list = []\n",
    "        for source in sources['sources']:\n",
    "            id_list.append(source['src_id'])\n",
    "            \n",
    "        quantile,quantile_net,error,error_net = quantile_compute_groups(2,12,id_list)\n",
    "        sources['meta'][0]['quantile'] = []\n",
    "        sources['meta'][0]['quantile_net'] = []\n",
    "        sources['meta'][0]['error_net'] = []\n",
    "        #print(quantile)\n",
    "        sources['meta'][0]['quantile'].append({\n",
    "            'q25': quantile[0],\n",
    "            'q33': quantile[1],\n",
    "            'q50': quantile[2],\n",
    "            'q67': quantile[3],\n",
    "            'q75': quantile[4]\n",
    "        })\n",
    "        sources['meta'][0]['quantile_net'].append({\n",
    "            'q25': quantile_net[0],\n",
    "            'q33': quantile_net[1],\n",
    "            'q50': quantile_net[2],\n",
    "            'q67': quantile_net[3],\n",
    "            'q75': quantile_net[4]\n",
    "        })\n",
    "        sources['meta'][0]['error_net'].append({\n",
    "            'q25': error_net[0],\n",
    "            'q33': error_net[1],\n",
    "            'q50': error_net[2],\n",
    "            'q67': error_net[3],\n",
    "            'q75': error_net[4]\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0613a34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from scipy.interpolate import interp1d\n",
    "def quantile_compute(min_energy,max_energy,src_id):\n",
    "    \n",
    "    # ---- User-provided filenames ----\n",
    "    spec_files = glob.glob('test_script/' + str(src_id) + '/*SRSPEC*.FTZ')         # Your main source spectrum (.pha)\n",
    "    bkg_files = glob.glob('test_script/' + str(src_id) + '/*BGSPEC*.FTZ')        # Background spectrum (.pha)\n",
    "    rmf_files = glob.glob('test_script/' + str(src_id) + '/*.rmf')          # Response Matrix File (RMF) with EBOUNDS\n",
    "    # arf_file = 'ancillary.arf'       # (Not used directly here for energies)\n",
    "    flag = False\n",
    "    for file in spec_files:\n",
    "        if 'PN' in file:\n",
    "            spec_file = file\n",
    "            flag = True\n",
    "    if not flag:\n",
    "        for file in spec_files:\n",
    "            if 'M1' or 'M2' in file:\n",
    "                spec_file = file\n",
    "    \n",
    "    for file in bkg_files:\n",
    "        if 'PN' in file:\n",
    "            bkg_file = file\n",
    "    if not flag:\n",
    "        for file in bkg_files:\n",
    "            if 'M1' or 'M2' in file:\n",
    "                bkg_file = file\n",
    "                \n",
    "    for file in rmf_files:\n",
    "        if 'epn' in file:\n",
    "            rmf_file = file\n",
    "    if not flag:\n",
    "        for file in rmf_files:\n",
    "            if 'm1' or 'm2' in file:\n",
    "                rmf_file = file\n",
    "    # ---- Read source spectrum ----\n",
    "    with fits.open(spec_file) as hdul:\n",
    "        src_counts = hdul[1].data['COUNTS']   # Array of counts per channel\n",
    "        src_backscal = hdul[1].header.get('BACKSCAL')\n",
    "        # spec_channels = hdul[1].data['CHANNEL']  # (optional) Channel index\n",
    "\n",
    "    # ---- Read background spectrum ----\n",
    "    with fits.open(bkg_file) as hdul:\n",
    "        bkg_counts = hdul[1].data['COUNTS']\n",
    "\n",
    "    # ---- Read RMF EBOUNDS to get energy bin edges ----\n",
    "    with fits.open(rmf_file) as hdul:\n",
    "        ebounds = hdul['EBOUNDS'].data\n",
    "        e_min = ebounds['E_MIN']    # Lower bound of each energy bin (keV)\n",
    "        e_max = ebounds['E_MAX']    # Upper bound of each energy bin (keV)\n",
    "\n",
    "    # ---- Compute net counts, taking care not to allow negative photons ----\n",
    "    E_lo = min_energy\n",
    "    E_up = max_energy\n",
    "    net_counts = np.maximum(src_counts - bkg_counts, 0)\n",
    "    a_scal = src_backscal/bkg_backscal\n",
    "    a_scal = src_backscal/bkg_backscal\n",
    "    # ---- Calculate midpoints for each bin ----\n",
    "    energy_midpoints = (e_min + e_max) / 2  # In keV\n",
    "    energy_mask = (energy_midpoints >= E_lo) & (energy_midpoints <= E_up)\n",
    "    energy_midpoints_cut = energy_midpoints[energy_mask]\n",
    "    src_counts = src_counts[energy_mask]\n",
    "    bkg_counts = bkg_counts[energy_mask]\n",
    "    net_counts = src_counts - bkg_counts * a_scal\n",
    "    net_counts = np.floor(net_counts).astype(int)   # round down to integers\n",
    "    net_counts[net_counts < 0] = 0                  # set negative counts to zero\n",
    "    net_energies = np.repeat(energy_midpoints_cut, net_counts)\n",
    "    net_energies_sorted = np.sort(net_energies)\n",
    "    # ---- Save or print as needed ----\n",
    "    #print(net_energies_sorted[:20])       # Show first 20 for verification\n",
    "    print(f\"Total photons after background subtraction: {len(net_energies_sorted)}\")\n",
    "    # np.savetxt('ordered_energies.txt', photon_energies_sorted)\n",
    "    \n",
    "    '''if len(sorted_energies) <= 2:\n",
    "        print('Error')\n",
    "        quantile = [0,0,0,0,0]\n",
    "        return quantile\n",
    "    '''\n",
    "    N = len(net_energies_sorted)\n",
    "    #Quantile Levels\n",
    "    x = [25,33,50,67,75]\n",
    "    minimum = 2\n",
    "    quantile = []\n",
    "    error = []\n",
    "    quantile_net = []\n",
    "    error_net = []\n",
    "    for percent in x:\n",
    "        if percent == 33 or percent == 50 or percent == 67:\n",
    "            minimum = 2\n",
    "        else:\n",
    "            minimum = 3\n",
    "        if len(net_energies_sorted) <= minimum:\n",
    "            quantile.append(0)\n",
    "            quantile_net.append(0)\n",
    "            error_net.append(0)\n",
    "        else:\n",
    "            quantile_net.append((quantile_net_compute(net_energies_sorted, percent, a_scal, N)-E_lo)/(E_up-E_lo))\n",
    "            error_net.append(error_bars(net_energies_sorted,percent,src_counts,bkg_counts,E_lo,E_up))\n",
    "            i = ((percent/100)*(2*N)+1)/2\n",
    "            if isinstance(i, int):\n",
    "                energy = net_energies_sorted(i)\n",
    "            else:\n",
    "                i_low = int(np.floor(i))\n",
    "                percent_low = (2*i_low-1)*100/(2*N)\n",
    "                percent_high = (2*i_low+1)*100/(2*N)\n",
    "                if i_low+1 > len(net_energies_sorted)-1:\n",
    "                    energy = (net_energies_sorted[i_low])\n",
    "                else:\n",
    "                    energy = (net_energies_sorted[i_low] + net_energies_sorted[i_low+1])/2\n",
    "\n",
    "            quantile.append((energy-E_lo)/(E_up-E_lo))\n",
    "    \n",
    "    return quantile, quantile_net,error,error_net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06f7bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import interp1d\n",
    "def quantile_net_compute(net_energies_sorted, x, a_scal, N):\n",
    "    energy_grid = np.unique(net_energies_sorted)\n",
    "    IC_above = N - np.searchsorted(net_energies_sorted, energy_grid, side='right')\n",
    "    IC_below = np.searchsorted(net_energies_sorted, energy_grid, side='left')\n",
    "    x_list = (N + IC_above - IC_below) / (2 * N)\n",
    "    x_unique, idx = np.unique(x_list, return_index=True)\n",
    "    energy_unique = energy_grid[idx]\n",
    "    interp_func = interp1d(x_unique[::-1], energy_unique[::-1], bounds_error=False, fill_value=\"extrapolate\")\n",
    "    monotonic_increasing = np.all(np.diff(x_unique) > 0)\n",
    "    #print(monotonic_increasing)\n",
    "    #print(f\"x min: {np.min(x_unique)}, max: {np.max(x_unique)}\")\n",
    "    #print('Net counts:', N)\n",
    "    #print('Energy grid:', energy_grid.min(), energy_grid.max())\n",
    "    #print('IC_above:', IC_above[:5], '... IC_below:', IC_below[:5])\n",
    "    #print(interp_func(1-(x/100)))\n",
    "    return interp_func(1-(x/100))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d56acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.integrate as integrate\n",
    "import scipy.special as special\n",
    "from scipy.integrate import quad\n",
    "import math\n",
    "from scipy.special import gammaln\n",
    "def error_bars(sorted_energies,x,src_counts,bkg_counts,E_lo,E_up):\n",
    "    x = x/100\n",
    "    N = len(sorted_energies)\n",
    "    #print(N)\n",
    "    M = N*x+0.5\n",
    "    a = M-1\n",
    "    #print(a)\n",
    "    b = N-M\n",
    "    W = 0\n",
    "    C1 = 0\n",
    "    C2 = 0\n",
    "    #print(a,b,N)\n",
    "    factor = math.sqrt(1+(np.sum(bkg_counts)/np.sum(src_counts)))\n",
    "    def integrand(t,a,b):\n",
    "        return t**(a-1)*((1-t)**(b-1))\n",
    "\n",
    "    def beta_prefactor(a, b):\n",
    "        return np.exp(gammaln(a + b) - gammaln(a) - gammaln(b))\n",
    "    def B(a,b,i):\n",
    "        y = i/N\n",
    "        return beta_prefactor(a,b)*quad(integrand, 0, y, args=(a,b))[0]\n",
    "    \n",
    "    minimum = 0\n",
    "    if x == 50:\n",
    "        minimum = 3\n",
    "    elif x == 33 or x == 67:\n",
    "        minimum = 5\n",
    "    else:\n",
    "        minimum = 6\n",
    "    if len(sorted_energies) < minimum:\n",
    "        return 0\n",
    "    else:\n",
    "        for i,photon in enumerate(sorted_energies):\n",
    "            W = B(a,b,i+1)-B(a,b,i)\n",
    "            C1 = C1+W*photon\n",
    "            C2 = C2+W*(photon**2)\n",
    "        #print(C1, C2)\n",
    "        variance = C2 - C1**2\n",
    "        if variance > 0:\n",
    "            if N < 30:\n",
    "                sigma_Q = factor*math.sqrt(C2-C1**2)\n",
    "                if ((sigma_Q)/(E_up-E_lo)) > 1:\n",
    "                    print(((factor*math.sqrt(C2-C1**2))/(E_up-E_lo)))\n",
    "                    print(f\"N={N}, factor={factor:.2f}, variance={variance:.4f}, sigma_E={math.sqrt(variance):.4f}, sigma_Q={sigma_Q:.4f}\")\n",
    "                return ((sigma_Q)/(E_up-E_lo))\n",
    "            else: \n",
    "                sigma_Q = math.sqrt(C2-C1**2)\n",
    "                if ((sigma_Q)/(E_up-E_lo)) > 1:\n",
    "                    print((sigma_Q)/(E_up-E_lo))\n",
    "                    print(f\"N={N}, factor={factor:.2f}, variance={variance:.4f}, sigma_E={math.sqrt(variance):.4f}, sigma_Q={sigma_Q:.4f}\")\n",
    "                return ((sigma_Q)/(E_up-E_lo))\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f175858",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from scipy.interpolate import interp1d\n",
    "def quantile_compute_groups(min_energy,max_energy,src_id_list):\n",
    "    E_lo = min_energy\n",
    "    E_up = max_energy\n",
    "    src_cts_total = 0\n",
    "    bkg_cts_total = 0\n",
    "    rebinned = []\n",
    "    for src_id in src_id_list:\n",
    "        spec_files = glob.glob('test_script/' + str(src_id) + '/*SRSPEC*.FTZ')         # Your main source spectrum (.pha)\n",
    "        bkg_files = glob.glob('test_script/' + str(src_id) + '/*BGSPEC*.FTZ')        # Background spectrum (.pha)\n",
    "        rmf_files = glob.glob('test_script/' + str(src_id) + '/*.rmf') \n",
    "        pn_flag = False\n",
    "        for file in rmf_files:\n",
    "            if 'PN' in file:\n",
    "                pn_flag = True\n",
    "                rmf_file = file\n",
    "                with fits.open(rmf_file) as hdul:\n",
    "                    ebounds = hdul['EBOUNDS'].data\n",
    "                    e_min = ebounds['E_MIN']    # Lower bound of each energy bin (keV)\n",
    "                    e_max = ebounds['E_MAX']    # Upper bound of each energy bin (keV)\n",
    "                if (np.max(e_min) >= E_up) & (np.min(e_min) <= E_lo) :\n",
    "                    common_energy_grid = e_min[(e_min >= E_lo) & (e_min <= E_up)]\n",
    "                elif (np.max(e_min) < E_up) & (np.min(e_min) > E_lo):\n",
    "                    common_energy_grid = e_min\n",
    "                    common_energy_grid.add(e_max[len(e_max)-1])\n",
    "                elif np.min(e_min) > E_lo:\n",
    "                    common_energy_grid = e_min[e_min <= E_up]\n",
    "                elif np.max(e_min) < E_up:\n",
    "                    common_energy_grid = e_min[e_min >= E_lo]\n",
    "                rebinned = np.zeros(len(common_energy_grid)-1)\n",
    "                rebinned_src = np.zeros(len(common_energy_grid)-1)\n",
    "                break\n",
    "        if pn_flag:\n",
    "            break\n",
    "                \n",
    "    if not pn_flag:\n",
    "        rmf_files = glob.glob('test_script/' + str(src_id_list[0]) + '/*.rmf') \n",
    "        rmf_file = rmf_files[0]\n",
    "        with fits.open(rmf_file) as hdul:\n",
    "                ebounds = hdul['EBOUNDS'].data\n",
    "                e_min = ebounds['E_MIN']    # Lower bound of each energy bin (keV)\n",
    "                e_max = ebounds['E_MAX']    # Upper bound of each energy bin (keV)\n",
    "        if (np.max(e_min) >= E_up) & (np.min(e_min) <= E_lo) :\n",
    "            common_energy_grid = e_min[(e_min >= E_lo) & (e_min <= E_up)]\n",
    "        elif (np.max(e_min) < E_up) & (np.min(e_min) > E_lo):\n",
    "            common_energy_grid = e_min\n",
    "            common_energy_grid.add(e_max[len(e_max)-1])\n",
    "        elif np.min(e_min) > E_lo:\n",
    "            common_energy_grid = e_min[e_min <= E_up]\n",
    "        elif np.max(e_min) < E_up:\n",
    "            common_energy_grid = e_min[e_min >= E_lo]\n",
    "        rebinned = np.zeros(len(common_energy_grid)-1)\n",
    "        rebinned_src = np.zeros(len(common_energy_grid)-1)\n",
    "            \n",
    "    for src_id in src_id_list:    \n",
    "        # ---- User-provided filenames ----\n",
    "        spec_files = glob.glob('test_script/' + str(src_id) + '/*SRSPEC*.FTZ')         # Your main source spectrum (.pha)\n",
    "        bkg_files = glob.glob('test_script/' + str(src_id) + '/*BGSPEC*.FTZ')        # Background spectrum (.pha)\n",
    "        rmf_files = glob.glob('test_script/' + str(src_id) + '/*.rmf')          # Response Matrix File (RMF) with EBOUNDS\n",
    "        # arf_file = 'ancillary.arf'       # (Not used directly here for energies)\n",
    "        flag = False\n",
    "        for file in spec_files:\n",
    "            if 'PN' in file:\n",
    "                spec_file = file\n",
    "                flag = True\n",
    "        if not flag:\n",
    "            for file in spec_files:\n",
    "                if ('m1' in file.lower()) or ('m2' in file.lower()):\n",
    "                    spec_file = file\n",
    "\n",
    "        for file in bkg_files:\n",
    "            if 'PN' in file:\n",
    "                bkg_file = file\n",
    "        if not flag:\n",
    "            for file in bkg_files:\n",
    "                if ('m1' in file.lower()) or ('m2' in file.lower()):\n",
    "                    bkg_file = file\n",
    "\n",
    "        for file in rmf_files:\n",
    "            if 'epn' in file:\n",
    "                rmf_file = file\n",
    "        if not flag:\n",
    "            for file in rmf_files:\n",
    "                if ('m1' in file.lower()) or ('m2' in file.lower()):\n",
    "                    rmf_file = file\n",
    "        # ---- Read source spectrum ----\n",
    "        with fits.open(spec_file) as hdul:\n",
    "            src_counts = hdul[1].data['COUNTS']   # Array of counts per channel\n",
    "            src_backscal = hdul[1].header.get('BACKSCAL')\n",
    "            # spec_channels = hdul[1].data['CHANNEL']  # (optional) Channel index\n",
    "\n",
    "        # ---- Read background spectrum ----\n",
    "        with fits.open(bkg_file) as hdul:\n",
    "            bkg_counts = hdul[1].data['COUNTS']\n",
    "            bkg_backscal = hdul[1].header.get('BACKSCAL')\n",
    "\n",
    "        # ---- Read RMF EBOUNDS to get energy bin edges ----\n",
    "        with fits.open(rmf_file) as hdul:\n",
    "            ebounds = hdul['EBOUNDS'].data\n",
    "            e_min = ebounds['E_MIN']    # Lower bound of each energy bin (keV)\n",
    "            e_max = ebounds['E_MAX']    # Upper bound of each energy bin (keV)\n",
    "\n",
    "        # ---- Compute net counts, taking care not to allow negative photons ----\n",
    "        a_scal = src_backscal/bkg_backscal\n",
    "        # ---- Calculate midpoints for each bin ----\n",
    "        energy_midpoints = (e_min + e_max) / 2  # In keV\n",
    "        energy_mask = (energy_midpoints >= E_lo) & (energy_midpoints <= E_up)\n",
    "        energy_midpoints_cut = energy_midpoints[energy_mask]\n",
    "        src_counts = src_counts[energy_mask]\n",
    "        bkg_counts = bkg_counts[energy_mask]\n",
    "        src_cts_total = src_cts_total + src_counts\n",
    "        bkg_cts_total = bkg_cts_total + bkg_counts\n",
    "        net_counts = src_counts - bkg_counts * a_scal\n",
    "        net_counts = np.floor(net_counts).astype(int)   # round down to integers\n",
    "        src_counts = np.floor(src_counts).astype(int)\n",
    "        net_counts[net_counts < 0] = 0                  # set negative counts to zero\n",
    "        src_counts[src_counts < 0] = 0\n",
    "        #net_energies = np.repeat(energy_midpoints_cut, net_counts)\n",
    "        #net_energies_sorted = np.sort(net_energies)\n",
    "        rebinned = rebinned + rebin_to_grid(energy_midpoints_cut,net_counts,common_energy_grid)\n",
    "        rebinned_src = rebinned_src + rebin_to_grid(energy_midpoints_cut,src_counts,common_energy_grid)\n",
    "    \n",
    "    energy_midpoints = (common_energy_grid[:-1] + common_energy_grid[1:]) / 2\n",
    "    net_energies = np.repeat(energy_midpoints, rebinned.astype(int))\n",
    "    src_energies = np.repeat(energy_midpoints, rebinned_src.astype(int))\n",
    "    net_energies_sorted = np.sort(net_energies)\n",
    "    src_energies_sorted = np.sort(src_energies)\n",
    "    \n",
    "    # ---- Save or print as needed ----\n",
    "    #print(net_energies_sorted[:20])       # Show first 20 for verification\n",
    "    print(f\"Total photons after background subtraction: {len(net_energies_sorted)}\")\n",
    "    # np.savetxt('ordered_energies.txt', photon_energies_sorted)\n",
    "    \n",
    "    '''if len(sorted_energies) <= 2:\n",
    "        print('Error')\n",
    "        quantile = [0,0,0,0,0]\n",
    "        return quantile\n",
    "    '''\n",
    "    N = len(net_energies_sorted)\n",
    "    #Quantile Levels\n",
    "    x = [25,33,50,67,75]\n",
    "    minimum = 2\n",
    "    quantile = []\n",
    "    error = []\n",
    "    quantile_net = []\n",
    "    error_net = []\n",
    "    for percent in x:\n",
    "        if percent == 33 or percent == 50 or percent == 67:\n",
    "            minimum = 2\n",
    "        else:\n",
    "            minimum = 3\n",
    "        if len(net_energies_sorted) <= minimum:\n",
    "            quantile.append(0)\n",
    "            quantile_net.append(0)\n",
    "            error_net.append(0)\n",
    "        else:\n",
    "            quantile_net.append((quantile_net_compute(net_energies_sorted, percent, a_scal, N)-E_lo)/(E_up-E_lo))\n",
    "            error_net.append(error_bars(src_energies_sorted,percent,src_cts_total,bkg_cts_total,E_lo,E_up))\n",
    "            i = ((percent/100)*(2*N)+1)/2\n",
    "            if i.is_integer():\n",
    "                energy = net_energies_sorted[int(i)]\n",
    "            else:\n",
    "                i_low = int(np.floor(i))\n",
    "                percent_low = (2*i_low-1)*100/(2*N)\n",
    "                percent_high = (2*i_low+1)*100/(2*N)\n",
    "                if i_low+1 > len(net_energies_sorted)-1:\n",
    "                    energy = (net_energies_sorted[i_low])\n",
    "                else:\n",
    "                    energy = (net_energies_sorted[i_low] + net_energies_sorted[i_low+1])/2\n",
    "\n",
    "            quantile.append((energy-E_lo)/(E_up-E_lo))\n",
    "    \n",
    "    return quantile, quantile_net,error,error_net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea5f204",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rebin_to_grid(channel_energies, counts, common_grid_edges):\n",
    "    # channel_energies = centers or midpoints (or use bin min/max for precise binning)\n",
    "    # counts = total events in each channel\n",
    "    rebinned, _ = np.histogram(channel_energies, bins=common_grid_edges, weights=counts)\n",
    "    return rebinned"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
